{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Sheet"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Sheet",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "pip install kneed"
   ],
   "execution_count":1,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Collecting kneed\r\n",
      "  Downloading kneed-0.8.5-py3-none-any.whl.metadata (5.5 kB)\r\n",
      "Requirement already satisfied: numpy>=1.14.2 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from kneed) (1.24.4)\r\n",
      "Requirement already satisfied: scipy>=1.0.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from kneed) (1.10.1)\r\n",
      "Downloading kneed-0.8.5-py3-none-any.whl (10 kB)\r\n",
      "Installing collected packages: kneed\r\n",
      "Successfully installed kneed-0.8.5\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"OqNR8qBgSZja8OlpWefoYI",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "pip install openpyxl"
   ],
   "execution_count":2,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Collecting openpyxl\r\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting et-xmlfile (from openpyxl)\r\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\r\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\r\n",
      "Installing collected packages: et-xmlfile, openpyxl\r\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"XUDcjyQe2BBBhi4xqGwN7c",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import euclidean"
   ],
   "execution_count":3,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"9PPUdmkbRq5K3aZefIg9Bu",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Part 1: Data Preparation \n",
    "### Adjust the bond prices to mitigate the impact of the benchmark bond roll."
   ],
   "outputs":[
    {
     "data":{
      "application\/datalore_markdown_expressions+json":{
       "expressions":{}
      }
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"FxbEU0b1iwFkCFcDStcftK",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "tr_dm = pd.read_excel('GENERIC BOND PRICE.xlsx', sheet_name='DM_PRICE')\n",
    "tr_dm.Dates = pd.to_datetime(tr_dm.Dates)\n",
    "tr_dm = tr_dm.set_index('Dates')"
   ],
   "execution_count":5,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"na8Z7HraSCUpsgtAhiH9El",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def calculate_dirty_price(clean_price, coupon, days_since_last_coupon):\n",
    "    daily_coupon = coupon \/ 365  \n",
    "    accrued_interest = daily_coupon * days_since_last_coupon\n",
    "    return clean_price + accrued_interest\n",
    "\n",
    "def calculate_days_since_last_coupon(date, last_coupon_date):\n",
    "    return (date - last_coupon_date).days\n",
    "\n",
    "def find_first_coupon_date(coupon_series):\n",
    "    coupon_changes = coupon_series.diff().dropna()\n",
    "    if len(coupon_changes) > 0:\n",
    "        first_change_date = coupon_changes.index[0]\n",
    "        return first_change_date - pd.Timedelta(days=180)  # All coupon paid 6 months before the change\n",
    "    else:\n",
    "        return coupon_series.index[0] \n",
    "    "
   ],
   "execution_count":6,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"qCpLGNyRVbpXvM2HxK3bzl",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "dirty_price_series = {}\n",
    "for column in tr_dm.columns:\n",
    "    if column.endswith('Govt'):\n",
    "        clean_price_series = tr_dm[column]\n",
    "        coupon_series = tr_dm[f\"{column} CPN\"]\n",
    "\n",
    "        first_coupon_date = find_first_coupon_date(coupon_series)\n",
    "        last_coupon_date = first_coupon_date\n",
    "        current_coupon = coupon_series.iloc[0]\n",
    "        \n",
    "        dirty_prices = []\n",
    "        \n",
    "        for date, clean_price in clean_price_series.items():\n",
    "            if coupon_series[date] != current_coupon:\n",
    "                last_coupon_date = date - pd.Timedelta(days=180)\n",
    "                current_coupon = coupon_series[date]\n",
    "            \n",
    "            days_since_last_coupon = calculate_days_since_last_coupon(date, last_coupon_date)\n",
    "            \n",
    "            dirty_price = calculate_dirty_price(clean_price, current_coupon, days_since_last_coupon)\n",
    "            dirty_prices.append(dirty_price)\n",
    "\n",
    "            if days_since_last_coupon >= 180:\n",
    "                last_coupon_date = date\n",
    "\n",
    "        tr_dm[f\"{column}_Dirty\"] = pd.Series(dirty_prices, index=clean_price_series.index)"
   ],
   "execution_count":7,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"I9Td6deyEsn0KKhefI4Gkm",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "for bond_col in tr_dm.columns:\n",
    "    if bond_col.endswith('Dirty'): \n",
    "        coupon_col = bond_col.replace('Govt_Dirty', 'Govt CPN')\n",
    "        bond_col = bond_col.replace('_Dirty','')\n",
    "        \n",
    "        tr_dm[f\"{bond_col} Adjusted\"] = tr_dm[bond_col]\n",
    "        \n",
    "        coupon_changes = tr_dm[coupon_col].diff().fillna(0) != 0\n",
    "\n",
    "        for change_date in tr_dm.index[coupon_changes]:\n",
    "\n",
    "            price_on_change = tr_dm.loc[change_date, bond_col]\n",
    "            \n",
    "            previous_price = tr_dm.loc[tr_dm.index[tr_dm.index.get_loc(change_date) - 1], bond_col]\n",
    "            \n",
    "            price_diff = price_on_change - previous_price\n",
    "            \n",
    "            tr_dm.loc[change_date:, f\"{bond_col} Adjusted\"] -= price_diff\n",
    "\n",
    "        tr_dm[f\"{bond_col} Adjusted Return\"] = tr_dm[f\"{bond_col} Adjusted\"].pct_change()\n",
    "\n",
    "\n",
    "# col_adj = [i for i in tr_dm.columns if (i.endswith('Adjusted') or i.endswith('Adjusted Return'))]\n",
    "\n",
    "col_adj_price_cpn = [i for i in tr_dm.columns if (i.endswith('Adjusted') or i.endswith('CPN'))]\n",
    "col_adj_price = [i for i in tr_dm.columns if i.endswith('Adjusted')]\n",
    "\n",
    "col_adj_return = [i for i in tr_dm.columns if i.endswith('Adjusted Return')]\n",
    "\n",
    "tr_dm_net = tr_dm[col_adj_return].fillna(0)\n",
    "\n",
    "tr_dm_net_price = tr_dm[col_adj_price].fillna(0)\n",
    "tr_dm_for_shock = tr_dm[col_adj_price_cpn]"
   ],
   "execution_count":8,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"s5S2f174w5nndoxNp8Ls5U",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "tr_dm_for_shock"
   ],
   "execution_count":9,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>GTFRF10Y Govt CPN<\/th>\n",
       "      <th>GTITL10Y Govt CPN<\/th>\n",
       "      <th>GTSEK10Y Govt CPN<\/th>\n",
       "      <th>GTCHF10Y Govt CPN<\/th>\n",
       "      <th>GT10 Govt CPN<\/th>\n",
       "      <th>GTCAD10Y Govt CPN<\/th>\n",
       "      <th>GTAUD10Y Govt CPN<\/th>\n",
       "      <th>GTNZD10Y Govt CPN<\/th>\n",
       "      <th>GTGBP10Y Govt CPN<\/th>\n",
       "      <th>GTDEM10Y Govt CPN<\/th>\n",
       "      <th>...<\/th>\n",
       "      <th>GTITL10Y Govt Adjusted<\/th>\n",
       "      <th>GTSEK10Y Govt Adjusted<\/th>\n",
       "      <th>GTCHF10Y Govt Adjusted<\/th>\n",
       "      <th>GT10 Govt Adjusted<\/th>\n",
       "      <th>GTCAD10Y Govt Adjusted<\/th>\n",
       "      <th>GTAUD10Y Govt Adjusted<\/th>\n",
       "      <th>GTNZD10Y Govt Adjusted<\/th>\n",
       "      <th>GTGBP10Y Govt Adjusted<\/th>\n",
       "      <th>GTDEM10Y Govt Adjusted<\/th>\n",
       "      <th>GTJPY10Y Govt Adjusted<\/th>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>Dates<\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-06-20<\/th>\n",
       "      <td>1.75<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>1.50<\/td>\n",
       "      <td>1.25<\/td>\n",
       "      <td>2.500<\/td>\n",
       "      <td>2.5<\/td>\n",
       "      <td>2.75<\/td>\n",
       "      <td>5.50<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>1.5<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>107.220<\/td>\n",
       "      <td>97.285<\/td>\n",
       "      <td>105.100<\/td>\n",
       "      <td>99.070312<\/td>\n",
       "      <td>101.820<\/td>\n",
       "      <td>92.385<\/td>\n",
       "      <td>107.542<\/td>\n",
       "      <td>95.875<\/td>\n",
       "      <td>101.440<\/td>\n",
       "      <td>100.122<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2014-06-23<\/th>\n",
       "      <td>1.75<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>1.50<\/td>\n",
       "      <td>1.25<\/td>\n",
       "      <td>2.500<\/td>\n",
       "      <td>2.5<\/td>\n",
       "      <td>2.75<\/td>\n",
       "      <td>5.50<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>1.5<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>107.615<\/td>\n",
       "      <td>97.294<\/td>\n",
       "      <td>105.190<\/td>\n",
       "      <td>98.890625<\/td>\n",
       "      <td>101.464<\/td>\n",
       "      <td>92.262<\/td>\n",
       "      <td>107.414<\/td>\n",
       "      <td>96.050<\/td>\n",
       "      <td>101.630<\/td>\n",
       "      <td>100.150<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2014-06-24<\/th>\n",
       "      <td>1.75<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>2.50<\/td>\n",
       "      <td>1.25<\/td>\n",
       "      <td>2.500<\/td>\n",
       "      <td>2.5<\/td>\n",
       "      <td>2.75<\/td>\n",
       "      <td>5.50<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>1.5<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>107.805<\/td>\n",
       "      <td>97.294<\/td>\n",
       "      <td>105.170<\/td>\n",
       "      <td>99.304688<\/td>\n",
       "      <td>101.908<\/td>\n",
       "      <td>92.694<\/td>\n",
       "      <td>107.249<\/td>\n",
       "      <td>96.085<\/td>\n",
       "      <td>101.660<\/td>\n",
       "      <td>100.169<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2014-06-25<\/th>\n",
       "      <td>1.75<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>2.50<\/td>\n",
       "      <td>1.25<\/td>\n",
       "      <td>2.500<\/td>\n",
       "      <td>2.5<\/td>\n",
       "      <td>2.75<\/td>\n",
       "      <td>5.50<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>1.5<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>108.070<\/td>\n",
       "      <td>97.873<\/td>\n",
       "      <td>105.580<\/td>\n",
       "      <td>99.468750<\/td>\n",
       "      <td>102.022<\/td>\n",
       "      <td>93.084<\/td>\n",
       "      <td>107.324<\/td>\n",
       "      <td>96.785<\/td>\n",
       "      <td>102.166<\/td>\n",
       "      <td>100.245<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2014-06-26<\/th>\n",
       "      <td>1.75<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>2.50<\/td>\n",
       "      <td>1.25<\/td>\n",
       "      <td>2.500<\/td>\n",
       "      <td>2.5<\/td>\n",
       "      <td>2.75<\/td>\n",
       "      <td>5.50<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>1.5<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>108.120<\/td>\n",
       "      <td>98.165<\/td>\n",
       "      <td>105.620<\/td>\n",
       "      <td>99.734375<\/td>\n",
       "      <td>102.284<\/td>\n",
       "      <td>93.090<\/td>\n",
       "      <td>107.102<\/td>\n",
       "      <td>96.865<\/td>\n",
       "      <td>102.370<\/td>\n",
       "      <td>100.264<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2024-09-06<\/th>\n",
       "      <td>3.00<\/td>\n",
       "      <td>3.85<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>0.00<\/td>\n",
       "      <td>3.875<\/td>\n",
       "      <td>3.0<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>2.6<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>118.718<\/td>\n",
       "      <td>100.244<\/td>\n",
       "      <td>96.620<\/td>\n",
       "      <td>93.644531<\/td>\n",
       "      <td>102.557<\/td>\n",
       "      <td>96.858<\/td>\n",
       "      <td>114.218<\/td>\n",
       "      <td>78.787<\/td>\n",
       "      <td>96.922<\/td>\n",
       "      <td>101.281<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2024-09-09<\/th>\n",
       "      <td>3.00<\/td>\n",
       "      <td>3.85<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>0.00<\/td>\n",
       "      <td>3.875<\/td>\n",
       "      <td>3.0<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>2.6<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>118.755<\/td>\n",
       "      <td>100.450<\/td>\n",
       "      <td>96.558<\/td>\n",
       "      <td>93.722656<\/td>\n",
       "      <td>102.773<\/td>\n",
       "      <td>96.277<\/td>\n",
       "      <td>113.792<\/td>\n",
       "      <td>79.038<\/td>\n",
       "      <td>96.956<\/td>\n",
       "      <td>100.848<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10<\/th>\n",
       "      <td>3.00<\/td>\n",
       "      <td>3.85<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>0.00<\/td>\n",
       "      <td>3.875<\/td>\n",
       "      <td>3.0<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>2.6<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>119.071<\/td>\n",
       "      <td>100.842<\/td>\n",
       "      <td>96.737<\/td>\n",
       "      <td>94.191406<\/td>\n",
       "      <td>103.113<\/td>\n",
       "      <td>96.639<\/td>\n",
       "      <td>113.942<\/td>\n",
       "      <td>79.343<\/td>\n",
       "      <td>97.296<\/td>\n",
       "      <td>100.857<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2024-09-11<\/th>\n",
       "      <td>3.00<\/td>\n",
       "      <td>3.85<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>0.00<\/td>\n",
       "      <td>3.875<\/td>\n",
       "      <td>3.0<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>2.6<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>119.428<\/td>\n",
       "      <td>101.373<\/td>\n",
       "      <td>97.016<\/td>\n",
       "      <td>94.113281<\/td>\n",
       "      <td>102.962<\/td>\n",
       "      <td>97.112<\/td>\n",
       "      <td>114.432<\/td>\n",
       "      <td>79.826<\/td>\n",
       "      <td>97.471<\/td>\n",
       "      <td>101.260<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2024-09-12<\/th>\n",
       "      <td>3.00<\/td>\n",
       "      <td>3.85<\/td>\n",
       "      <td>2.25<\/td>\n",
       "      <td>0.00<\/td>\n",
       "      <td>3.875<\/td>\n",
       "      <td>3.0<\/td>\n",
       "      <td>3.75<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>4.25<\/td>\n",
       "      <td>2.6<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>119.384<\/td>\n",
       "      <td>101.242<\/td>\n",
       "      <td>96.880<\/td>\n",
       "      <td>93.847656<\/td>\n",
       "      <td>102.894<\/td>\n",
       "      <td>97.093<\/td>\n",
       "      <td>114.384<\/td>\n",
       "      <td>79.631<\/td>\n",
       "      <td>97.097<\/td>\n",
       "      <td>101.140<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>2670 rows × 22 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"229DTpOfyLuaFmnRHfV2NG",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Part 3: Identify Correlated Assets (Developed Market)\n",
    "### Step1 : PCA (cluster by the first principal component)"
   ],
   "outputs":[
    {
     "data":{
      "application\/datalore_markdown_expressions+json":{
       "expressions":{}
      }
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"TuVft2K7E4OGuQ0C6RGmSQ",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "def test_cointegration_in_clusters(data, cluster_dict):\n",
    "\n",
    "    cointegrated_pairs = []\n",
    "    for cluster_num in cluster_dict:\n",
    "        asset_names = cluster_dict[cluster_num]\n",
    "        asset_names = [i.replace(' Return', '') for i in asset_names]\n",
    "        \n",
    "        # Loop through each pair of assets in the cluster\n",
    "        for i in range(len(asset_names)):\n",
    "            for j in range(i + 1, len(asset_names)):\n",
    "                asset1 = asset_names[i]\n",
    "                asset2 = asset_names[j]\n",
    "             \n",
    "                series1 = data[asset1]\n",
    "                series2 = data[asset2]\n",
    "                \n",
    "                # Perform the Engle-Granger cointegration test\n",
    "                coint_t, p_value, _ = coint(series1, series2)\n",
    "                \n",
    "                # set a higher significant level (0.2) to avoid missing potential relationship\n",
    "                if p_value < 0.2:  \n",
    "                    cointegrated_pairs.append([asset1,asset2])\n",
    "                else:\n",
    "                    pass\n",
    "                    # print(f\"  {asset1} and {asset2} are NOT cointegrated (p-value: {p_value:.4f})\")\n",
    "\n",
    "    return cointegrated_pairs\n"
   ],
   "execution_count":11,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"OAaLctWBqKuHggcbLfy7DD",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "def noncoherent_pair_cluster(data, data_price): # input: return data, price data\n",
    "    # Step 1: Cluster by Principal component 1\n",
    "    scaler = StandardScaler()\n",
    "    asset_returns = pd.DataFrame(scaler.fit_transform(data), columns= data.columns)\n",
    "\n",
    "\n",
    "    # Calculate the loadings of bond returns on PCs\n",
    "    K = 1\n",
    "    pca = PCA(n_components=K)\n",
    "    pca.fit(asset_returns)\n",
    "    loadings = pca.components_.T\n",
    "\n",
    "    wcss = []\n",
    "    for k in range(1, 10):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(loadings)\n",
    "        # Inertia: Sum of squared distances to closest cluster center\n",
    "        wcss.append(kmeans.inertia_)  \n",
    "        \n",
    "    # Use the KneeLocator to detect the elbow point\n",
    "    kneedle = KneeLocator(range(1, 10), wcss, S=1.0, curve='convex', direction='decreasing')\n",
    "\n",
    "    # Get the optimal number of clusters\n",
    "    optimal_clusters = kneedle.elbow\n",
    "\n",
    "\n",
    "    # Clustering in the principal component space and using K-means to cluster different govt bonds\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(loadings)\n",
    "    asset_names = asset_returns.columns\n",
    "\n",
    "    cluster_dic = {}\n",
    "    for cluster in range(optimal_clusters):\n",
    "        cluster_assets = asset_names[clusters == cluster]\n",
    "        cluster_dic[cluster + 1] =cluster_assets\n",
    "\n",
    "\n",
    "    # Step 2: Find cointegration pairs\n",
    "    cointegrated_pairs = test_cointegration_in_clusters(data_price, cluster_dic)\n",
    "\n",
    "    # Step 3: Exclude pairs have similar PC2 and PC3\n",
    "    K = 3\n",
    "    pca = PCA(n_components=K)\n",
    "    pca.fit(asset_returns)\n",
    "    loadings = pca.components_.T\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "    loadings_df = pd.DataFrame(loadings, columns = ['PC1','PC2','PC3'], index = data.columns)\n",
    "\n",
    "    pc2_diff_list = [abs(loadings_df.loc[f\"{i} Return\", 'PC2'] - loadings_df.loc[f\"{j} Return\", 'PC2']) for (i,j) in cointegrated_pairs]\n",
    "\n",
    "    threshold_pc2 = np.quantile(pc2_diff_list, 0.5)\n",
    "\n",
    "    coherent_pair = []\n",
    "    noncoherent_pair = []\n",
    "\n",
    "    for i in range(len(cointegrated_pairs)):\n",
    "        asset1 = cointegrated_pairs[i][0]\n",
    "        asset2 = cointegrated_pairs[i][1]\n",
    "\n",
    "        pc1_diff = abs(loadings_df.loc[f\"{asset1} Return\", 'PC1']  - loadings_df.loc[f\"{asset2} Return\", 'PC1'])\n",
    "        pc2_diff = abs(loadings_df.loc[f\"{asset1} Return\", 'PC2']  - loadings_df.loc[f\"{asset2} Return\", 'PC2'])\n",
    "\n",
    "        if pc2_diff < threshold_pc2:\n",
    "            coherent_pair.append((f'{asset1}', f'{asset2}'))\n",
    "            # print(asset1, asset2, pc1_diff, 'coherent')\n",
    "        else:\n",
    "            noncoherent_pair.append((f'{asset1}', f'{asset2}'))\n",
    "            # print(asset1, asset2, pc1_diff, 'noncoherent')\n",
    "            \n",
    "\n",
    "    return coherent_pair, noncoherent_pair\n"
   ],
   "execution_count":12,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"YTZ4KjUXOOGggR98zwiv3r",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Part 4: Naiive Rule-based Trading Algorithm"
   ],
   "outputs":[
    {
     "data":{
      "application\/datalore_markdown_expressions+json":{
       "expressions":{}
      }
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Nqxt5EA3IH5wClIWGl2rFr",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "The code below implements a rule-based pair trading strategy that is based on the 2nd principal component loadings of the asset pair to construct a market-neutral spread.\n",
    "\n",
    "Key Steps:\n",
    "1. Rolling PCA Calculation:\n",
    "\n",
    "Option 1: PCA is applied on the returns of only two selected assets. In this case, the first principal component (PC1) represents the common trend between the two selected assets.\n",
    "\n",
    "Option 2: PCA can be applied to all 11 bonds, which would make the PC1 represent the broader market trend for all treasury bonds. PC2 represents the specific factors that diverge from the broader market, allowing for more targeted trading opportunities.\n",
    "\n",
    "2. Rolling Loadings:\n",
    "\n",
    "The PCA loadings are calculated on a rolling basis to avoid look-ahead bias over time. The strategy focuses on the loadings for the second principal component (PC2), which reflects the bond-specific factors that are relatively insensitive (orthogonal) to broader market movements captured by PC1.\n",
    "The code extracts the rolling loadings of the selected assets on PC2 and uses these loadings to construct the spread.\n",
    "\n",
    "3. Spread Construction:\n",
    "\n",
    "The spread is calculated as a linear combination of the two assets' returns weighted by their PC2 loadings.\n",
    "Then the weights are normalized based on the total absolute weight of the two assets to ensure that the portfolio remains balanced.\n",
    "\n",
    "$ \\text{Spread}  = w1 * \\text{Asset 1 Return} - w2 * \\text{Asset 2 Return}$\n",
    "\n",
    "$ w1 = 1 $\n",
    "\n",
    "\n",
    "$ w2 = w1 * \\frac{\\sigma 1}{\\sigma 2} * \\frac{PC loading Asset 1}{PC Loading Asset 2} $\n",
    "\n",
    "\n",
    "\n",
    "1. Trading Signals:\n",
    "\n",
    "Buy Signal: Generated when the z-score of the spread (number of standard deviations the spread deviates from its rolling mean) falls below a specified lower threshold. This implies the spread has diverged significantly, and the strategy goes long on the spread, expecting a reversal.\n",
    "Sell Signal: Generated when the z-score rises above the upper threshold, indicating the spread has widened significantly. The strategy goes short on the spread, expecting a convergence.\n",
    "Exit Signal: The strategy closes the positions when the spread reverts to a level closer to the mean, as defined by a close threshold.\n",
    "\n",
    "5. Z-Score and Thresholds:\n",
    "\n",
    "The z-score is calculated based on the rolling mean and rolling standard deviation of the spread. This z-score is used to quantify the divergence of the spread from its historical average.\n",
    "The thresholds (upper, lower, and close) dictate when the strategy enters and exits positions.\n",
    "\n",
    "6. Performance Evaluation:\n",
    "\n",
    "The code tracks the cumulative returns of the pair trading strategy over time. It shifts the positions by one day to avoid look-ahead bias when calculating the returns for the next day.\n",
    "The final cumulative returns are plotted to visualize the strategy's performance for the selected asset pair."
   ],
   "outputs":[
    {
     "data":{
      "application\/datalore_markdown_expressions+json":{
       "expressions":{}
      }
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"EtKheX28n7YrMDQf952kP1",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def rolling_pca_loadings(data, window_size, num_components=2):\n",
    "    rolling_loadings = []\n",
    "    \n",
    "    for i in range(window_size, len(data) + 1):\n",
    "        window_data = data[i - window_size:i]\n",
    "        pca = PCA(n_components=num_components)\n",
    "        pca.fit(window_data)\n",
    "        loadings = pca.components_.T  \n",
    "        rolling_loadings.append(loadings)\n",
    "        \n",
    "    return np.array(rolling_loadings)\n",
    "\n",
    "def diff_sign_pc_loadings(data, window_size):\n",
    "    loadings = rolling_pca_loadings(data, window_size, num_components=2)\n",
    "    result = []\n",
    "    for matrix in loadings:\n",
    "        first_col = matrix[:, 0]\n",
    "        second_col = matrix[:, 1]\n",
    "        \n",
    "        if np.sign(first_col[0]) == np.sign(first_col[1]):\n",
    "            result.append(first_col)\n",
    "        else:\n",
    "            result.append(second_col)\n",
    "    \n",
    "    result_array = np.array(result)\n",
    "    \n",
    "    return result_array"
   ],
   "execution_count":15,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"I5JpXbpiVv7hdJGf73whLK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Non-coherent pair\n",
    "def rule_based_strategy_info(data, window_size, upper_threshold,close_threshold, asset1,asset2):\n",
    "\n",
    "    # 1. Rolling PCA Calculation:\n",
    "    # option1: apply pca on two assets: pc1 represents the common trend between asset1 and asset 2 only\n",
    "    asset1 = f'{asset1} Return'\n",
    "    asset2 = f'{asset2} Return'\n",
    "    selected_asset_returns = data[[f'{asset1}', f'{asset2}']]\n",
    "    \n",
    "    # 2. Rolling Loadings:\n",
    "    rolling_loadings = diff_sign_pc_loadings(selected_asset_returns, window_size)\n",
    "\n",
    "    pc_loading_asset1 = rolling_loadings[:, 0] \n",
    "    pc_loading_asset2 = rolling_loadings[:, 1] \n",
    "\n",
    "    # 3. Spread Construction:\n",
    "    \n",
    "    sigma_asset1 = data[f'{asset1}'].rolling(window=window_size).std().dropna()\n",
    "    sigma_asset2 = data[f'{asset2}'].rolling(window=window_size).std().dropna()\n",
    "    pc_loading_asset1 = pd.Series(pc_loading_asset1, index = sigma_asset1.index)\n",
    "    pc_loading_asset2 = pd.Series(pc_loading_asset2, index = sigma_asset2.index)\n",
    "\n",
    "    sigma_ratio = sigma_asset1 \/ sigma_asset2\n",
    "    \n",
    "    w1 = 1\n",
    "    w2 = (w1 * sigma_ratio * pc_loading_asset1)\/pc_loading_asset2\n",
    "    \n",
    "    spread = w1 * data[f'{asset1}'] - w2 * data[f'{asset2}']\n",
    "\n",
    "    # 4. Trading Signals:\n",
    "    rolling_mean = spread.rolling(window=window_size).mean()\n",
    "    rolling_std = spread.rolling(window=window_size).std()\n",
    "\n",
    "    # 5. Z-Score and Thresholds:\n",
    "    z_score = (spread - rolling_mean) \/ rolling_std\n",
    "    \n",
    "    lower_threshold = - upper_threshold\n",
    "    \n",
    "    positions = pd.DataFrame(index=data.index, columns=['Position','Holdings_w1','Holdings_w2'])\n",
    "    \n",
    "    # Enter signal (spread deviates from the mean)\n",
    "    positions['Position'] = np.where(z_score > upper_threshold, -1, 0)\n",
    "    positions['Position'] = np.where(z_score < lower_threshold, 1, positions['Position'])\n",
    "    \n",
    "    # Exit signal (spread reverts to the mean)\n",
    "    positions['Position'] = np.where((z_score > - close_threshold) & (positions['Position'] == 1), 0, positions['Position'])\n",
    "    positions['Position'] = np.where((z_score < close_threshold) & (positions['Position'] == -1), 0, positions['Position'])\n",
    "    \n",
    "    positions['Holdings_w1'] = positions['Position'] * w1\n",
    "    positions['Holdings_w2'] = positions['Position'] * w2\n",
    "\n",
    "\n",
    "    # 6. Performance Evaluation:\n",
    "    positions_shifted = positions.shift(1)  \n",
    "    returns = positions_shifted['Holdings_w1'] * data[f'{asset1}'] + positions_shifted['Holdings_w2'] * data[f'{asset2}']\n",
    "    initial_investment = 1\n",
    "    cumulative_returns = (1+ returns).cumprod() -1\n",
    "\n",
    "    return z_score, spread, returns, cumulative_returns, positions"
   ],
   "execution_count":16,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"6whJUtfEYlt8aFZBsH4Hjn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# coherent pair\n",
    "def rule_based_strategy(data, window_size, upper_threshold,close_threshold, asset1,asset2):\n",
    "\n",
    "    # 1. Rolling PCA Calculation:\n",
    "    # option1: apply pca on two assets: pc1 represents the common trend between asset1 and asset 2 only\n",
    "    asset1 = f'{asset1} Return'\n",
    "    asset2 = f'{asset2} Return'\n",
    "    selected_asset_returns = data[[f'{asset1}', f'{asset2}']]\n",
    "    \n",
    "    # 2. Rolling Loadings:\n",
    "    rolling_loadings = diff_sign_pc_loadings(selected_asset_returns, window_size)\n",
    "\n",
    "    pc_loading_asset1 = rolling_loadings[:, 0] \n",
    "    pc_loading_asset2 = rolling_loadings[:, 1] \n",
    "\n",
    "    # 3. Spread Construction:\n",
    "    \n",
    "    sigma_asset1 = data[f'{asset1}'].rolling(window=window_size).std().dropna()\n",
    "    sigma_asset2 = data[f'{asset2}'].rolling(window=window_size).std().dropna()\n",
    "    pc_loading_asset1 = pd.Series(pc_loading_asset1, index = sigma_asset1.index)\n",
    "    pc_loading_asset2 = pd.Series(pc_loading_asset2, index = sigma_asset2.index)\n",
    "\n",
    "    sigma_ratio = sigma_asset1 \/ sigma_asset2\n",
    "    \n",
    "    w1 = 1\n",
    "    w2 = (w1 * sigma_ratio * pc_loading_asset1)\/pc_loading_asset2\n",
    "    \n",
    "    spread = w1 * data[f'{asset1}'] - w2 * data[f'{asset2}']\n",
    "\n",
    "    # 4. Trading Signals:\n",
    "    rolling_mean = spread.rolling(window=window_size).mean()\n",
    "    rolling_std = spread.rolling(window=window_size).std()\n",
    "\n",
    "    # 5. Z-Score and Thresholds:\n",
    "    z_score = (spread - rolling_mean) \/ rolling_std\n",
    "    \n",
    "    lower_threshold = - upper_threshold\n",
    "    \n",
    "    positions = pd.DataFrame(index=data.index, columns=['Position','Holdings_w1','Holdings_w2'])\n",
    "    \n",
    "    positions['Position'] = np.where(z_score > upper_threshold, -1, 0)\n",
    "    positions['Position'] = np.where(z_score < lower_threshold, 1, positions['Position'])\n",
    "    \n",
    "    # Exit signal (spread reverts to the mean)\n",
    "    positions['Position'] = np.where((z_score > - close_threshold) & (positions['Position'] == 1), 0, positions['Position'])\n",
    "    positions['Position'] = np.where((z_score < close_threshold) & (positions['Position'] == -1), 0, positions['Position'])\n",
    "    \n",
    "    positions['Holdings_w1'] = positions['Position'] * w1\n",
    "    positions['Holdings_w2'] = positions['Position'] * w2\n",
    "\n",
    "\n",
    "    # 6. Performance Evaluation:\n",
    "    positions_shifted = positions.shift(1)  \n",
    "    returns = positions_shifted['Holdings_w1'] * data[f'{asset1}'] + positions_shifted['Holdings_w2'] * data[f'{asset2}']\n",
    "    initial_investment = 1\n",
    "    cumulative_returns = (1+ returns).cumprod() -1\n",
    "\n",
    "    return {\n",
    "    'positions':positions['Position'],\n",
    "    'z_score':z_score, \n",
    "    'cum_return':cumulative_returns\n",
    "    }"
   ],
   "execution_count":17,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"yIwOIiFivIeBnrCHKphv3n",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Grid search for the optimal parameters"
   ],
   "outputs":[
    {
     "data":{
      "application\/datalore_markdown_expressions+json":{
       "expressions":{}
      }
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"ixNDbv1uRxqyLsDNxbEAFw",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def optimize_parameters(data, noncoherent_pair, window_size_list, upper_threshold_list, close_threshold_list):\n",
    "\n",
    "    param_results = {}\n",
    "    pair_results = {}\n",
    "    \n",
    "    # Try each parameter combination\n",
    "    for window_size, upper_threshold, close_threshold in itertools.product(\n",
    "            window_size_list, upper_threshold_list, close_threshold_list):\n",
    "        \n",
    "        if close_threshold >= upper_threshold:\n",
    "            continue\n",
    "            \n",
    "        returns = []\n",
    "        pair_details = []\n",
    "        \n",
    "        for asset_pair in noncoherent_pair:\n",
    "            asset1 = asset_pair[0]#.replace(' Adjusted Return', '')\n",
    "            asset2 = asset_pair[1]#.replace(' Adjusted Return', '')\n",
    "            \n",
    "            # Get return for this pair with current parameters\n",
    "            result = rule_based_strategy(data, window_size, upper_threshold, close_threshold, asset1, asset2)\n",
    "            returns.append(result['cum_returns'][-1])\n",
    "            \n",
    "            pair_details.append({'asset1': asset1, 'asset2': asset2, 'return': result})\n",
    "        \n",
    "        # Store average return for this parameter combination\n",
    "        param_key = (window_size, upper_threshold, close_threshold)\n",
    "        param_results[param_key] = {'avg_return': np.mean(returns),'pair_details': pair_details}\n",
    "\n",
    "    \n",
    "    # Find best parameter combination\n",
    "    best_params = max(param_results.items(), key=lambda x: x[1]['avg_return'])\n",
    "    window_size, upper_threshold, close_threshold = best_params[0]\n",
    "    \n",
    "    \n",
    "    return upper_threshold, close_threshold, window_size\n"
   ],
   "execution_count":19,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"XRsls6Nfsnf8IH9smFPzm1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Part 5: Reinforcement Learning Based Strategy"
   ],
   "outputs":[
    {
     "data":{
      "application\/datalore_markdown_expressions+json":{
       "expressions":{}
      }
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"8u7rXTvSP7fwoHt4ULmkGb",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "To enhance the existing rule-based strategy for pair trading using a Deep Q-Network (DQN), we can replace the rule-based signals with a reinforcement learning (RL) approach. The purpose for RL step is to find 'when' to trade and 'how much' to trade by interacting with observation space, action space and reward space.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Teachnique: DQN Strategy\n",
    "\n",
    "Dynamic Pair Selection & Training Process\n",
    "1. Formation Period (Year T): Select correlated pairs using PCA\n",
    "2. Training Period (Year T+1): Train RL agent on selected pairs\n",
    "3. Testing Period (Year T+2): Test agent on newly generated pairs\n",
    "4. Repeat process by rolling forward one year for robust validation\n",
    "\n",
    "![image.png](attachment:.\/attachment:853841c4-393a-4b7c-870b-290257aa81f2.png)\n",
    "\n",
    "Model Architecture\n",
    "1. Deep Q-Network (DQN) for automated trading decisions \n",
    "2. State space includes normalized spread, z-score, position metrics\n",
    "3. Action space: Long (1), Neutral (0), Short (-1) positions\n",
    "4. Reward = Return - beta * (RL action - rule-based strategy action)\n",
    "\n",
    "Training Process:\n",
    "1. the agent learns optimal entry and exit points through extensive episodes with different pairs and market conditions\n",
    "2. Set the network with highest reward as target network, agent continuously learns to match or exceed this taret network performance\n",
    "3. the agent develop resilience to both regime changes and switching between different asset pairs, making it truly adaptive to market dynamics."
   ],
   "outputs":[
    {
     "data":{
      "application\/datalore_markdown_expressions+json":{
       "expressions":{}
      }
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"1t1thSUrnrGfT0e7KT2s4B",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# defines a new class called DQN that inherits from PyTorch's nn.Module. \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# todo: to be fine-tuned\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_sizes,activation = 'relu', dropout_rate=0.2):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            act_fn = nn.ReLU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            act_fn = nn.LeakyReLU(0.01)\n",
    "        elif activation == 'elu':\n",
    "            act_fn = nn.ELU()\n",
    "        elif activation == 'selu':\n",
    "            act_fn = nn.SELU()\n",
    "        elif activation == 'tanh':\n",
    "            act_fn = nn.Tanh()\n",
    "\n",
    "    \n",
    "        # Input layer\n",
    "        # Input layer with proper initialization\n",
    "        input_layer = nn.Linear(state_size, hidden_sizes[0])\n",
    "        nn.init.xavier_uniform_(input_layer.weight)\n",
    "        nn.init.zeros_(input_layer.bias)\n",
    "        self.layers.append(input_layer)\n",
    "        self.layers.append(act_fn)\n",
    "        \n",
    "        # Hidden layers with proper initialization\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            hidden_layer = nn.Linear(hidden_sizes[i-1], hidden_sizes[i])\n",
    "            nn.init.xavier_uniform_(hidden_layer.weight)\n",
    "            nn.init.zeros_(hidden_layer.bias)\n",
    "            self.layers.append(hidden_layer)\n",
    "            self.layers.append(act_fn)\n",
    "            \n",
    "            if dropout_rate > 0:\n",
    "                self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer with proper initialization\n",
    "        output_layer = nn.Linear(hidden_sizes[-1], action_size)\n",
    "        nn.init.xavier_uniform_(output_layer.weight)\n",
    "        nn.init.zeros_(output_layer.bias)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "# creates a named tuple to store experience replays. a convenient way to group related data.\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))"
   ],
   "execution_count":22,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"LUtuzOYD0iR1wibeNKYcJd",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "execution_count":23,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"smThOdR5upzpg1qwIoAcE3",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size,hidden_size, learning_rate=1e-5):\n",
    "        self.hidden_size= hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayBuffer(10000) # a replay buffer with a capacity of 10,000 experiences\n",
    "        self.gamma = 0.95    # discount factor\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.policy_net = DQN(state_size, action_size,hidden_size).to(self.device)\n",
    "        self.target_net = DQN(state_size, action_size,hidden_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters()) # update for policy network's weight\n",
    "        self.training_rmse_history = []  # Track training RMSE\n",
    "        self.q_value_history = []  # Store Q-value predictions\n",
    "        self.actual_returns_history = []  # Store actual returns\n",
    "        self.next_states = []\n",
    "\n",
    "        \n",
    "    def act(self, state, store_next_state = None):\n",
    "        if random.random() <= self.epsilon: # exploration (random act)\n",
    "            self.q_value_history.append(0)\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            action_values = self.policy_net(state)\n",
    "            action = action_values.max(1)[1].item()\n",
    "\n",
    "            q_value = action_values[0][action].item()\n",
    "\n",
    "            if not np.isnan(q_value):\n",
    "                self.q_value_history.append(q_value)\n",
    "            else:\n",
    "                self.q_value_history.append(0)\n",
    "\n",
    "            return action\n",
    "\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done) # add new experience to replay buffer\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size: # check if enough space in buffer\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(self.device)\n",
    "\n",
    "\n",
    "        # pass our batch of states through our policy network to get Q-values for all actions. \n",
    "        # use gather to select only the Q-values for the actions that were actually taken. This gives us our current estimate of the Q-values for our sampled state-action pairs\n",
    "        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # For each non-terminal next state, we compute the maximum Q-value using our target network. \n",
    "        # We use the target network (which is updated less frequently) to provide a more stable target for learning.\n",
    "        next_states = torch.FloatTensor([s for s in batch.next_state if s is not None]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if next_states.size(0) > 0:\n",
    "                next_actions = self.policy_net(next_states).max(1)[1]\n",
    "                next_state_values = self.target_net(next_states).gather(1, next_actions.unsqueeze(1))\n",
    "            else:\n",
    "                next_state_values = torch.zeros_like(current_q_values)\n",
    "\n",
    "        expected_q_values = (next_state_values * self.gamma) + reward_batch.unsqueeze(1)\n",
    "\n",
    "\n",
    "        # Weighted loss\n",
    "        mse_loss = F.mse_loss(current_q_values, expected_q_values)\n",
    "        # loss =  F.smooth_l1_loss(current_q_values, expected_q_values)\n",
    "        training_rmse  = torch.sqrt(mse_loss)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.training_rmse_history.append(training_rmse.item()) \n",
    "\n",
    "        return training_rmse.item()\n",
    "\n",
    "\n",
    "\n",
    "    # as we have more training samples, we don't need to explore too often\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def calculate_testing_rmse(self):\n",
    "        if not self.next_states:\n",
    "            return None\n",
    "            \n",
    "        with torch.no_grad():\n",
    " \n",
    "            next_state = torch.FloatTensor(self.next_states).to(self.device)\n",
    "            next_actions = self.policy_net(next_state).max(1)[1]\n",
    "            next_state_values = self.target_net(next_state).gather(1, next_actions.unsqueeze(1))\n",
    "\n",
    "            rewards = torch.FloatTensor(self.actual_returns_history).to(self.device)\n",
    "\n",
    "            target_q_values = rewards + (self.gamma * next_state_values.squeeze())\n",
    "            \n",
    "            predicted_q_values = torch.FloatTensor(self.q_value_history).to(self.device)\n",
    "        \n",
    "            bellman_error = torch.sqrt(F.mse_loss(predicted_q_values, target_q_values))\n",
    "            \n",
    "            return bellman_error.item()\n",
    "\n",
    "    \n",
    "    def reset_test_history(self):\n",
    "\n",
    "        self.q_value_history = []\n",
    "        self.actual_returns_history = []\n",
    "        self.next_states = []"
   ],
   "execution_count":24,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"gaC4FLYS2FDxpOeI65S98D",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class SinglePairTradingEnv:\n",
    "    def __init__(self, asset_returns_raw, asset1, asset2, upper_threshold, close_threshold, window_size, beta):\n",
    "        \"\"\"\n",
    "        Initialize single pair trading environment\n",
    "        \"\"\"\n",
    "        self.asset_returns_raw = asset_returns_raw\n",
    "        self.asset1 = asset1\n",
    "        self.asset2 = asset2\n",
    "        self.window_size = window_size\n",
    "        self.upper_threshold = upper_threshold\n",
    "        self.close_threshold = close_threshold\n",
    "        self.beta = beta\n",
    "        self.action_space = [-1, 0, 1]\n",
    "        \n",
    "        # Calculate spread for the pair\n",
    "        self.spread, self.w2 = self._calculate_spread()\n",
    "        \n",
    "        # Initialize training variables\n",
    "        self.reset()\n",
    "\n",
    "    def _calculate_spread(self):\n",
    "        \"\"\"Calculate spread for the pair\"\"\"\n",
    "        selected_asset_returns = self.asset_returns_raw[[f'{self.asset1} Return', f'{self.asset2} Return']]\n",
    "        \n",
    "        rolling_loadings = diff_sign_pc_loadings(selected_asset_returns, self.window_size)\n",
    "        pc_loading_asset1 = rolling_loadings[:, 0]\n",
    "        pc_loading_asset2 = rolling_loadings[:, 1]\n",
    "        \n",
    "        sigma_asset1 = self.asset_returns_raw[f'{self.asset1} Return'].rolling(window=self.window_size).std().dropna()\n",
    "        sigma_asset2 = self.asset_returns_raw[f'{self.asset2} Return'].rolling(window=self.window_size).std().dropna()\n",
    "        \n",
    "        pc_loading_asset1 = pd.Series(pc_loading_asset1, index=sigma_asset1.index)\n",
    "        pc_loading_asset2 = pd.Series(pc_loading_asset2, index=sigma_asset2.index)\n",
    "        sigma_ratio = sigma_asset1 \/ sigma_asset2\n",
    "        \n",
    "        w1 = 1\n",
    "        w2 = (w1 * sigma_ratio * pc_loading_asset2) \/ pc_loading_asset1\n",
    "        total_weight = abs(w1) + abs(w2)\n",
    "        w1 = w1 \/ total_weight\n",
    "        w2 = w2 \/ total_weight\n",
    "\n",
    "        spread = w1 * self.asset_returns_raw[f'{self.asset1} Return'] - w2 * self.asset_returns_raw[f'{self.asset2} Return']\n",
    "        \n",
    "        return spread, w2\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment state\"\"\"\n",
    "        # Reset position and portfolio\n",
    "        self.position = 0\n",
    "        self.previous_position = 0\n",
    "        self.portfolio_value = 1\n",
    "        self.steps_taken = 0\n",
    "        self.cumulative_return = [self.portfolio_value]\n",
    "        \n",
    "        # Reset tracking variables\n",
    "        self.portfolio_start_value = self.portfolio_value\n",
    "        self.last_position_change = None\n",
    "        self.position_start_value = None\n",
    "        self.return_list = []\n",
    "        self.action_history = []\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.current_step = self.window_size\n",
    "        \n",
    "        return self._get_normalized_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step in the environment\"\"\"\n",
    "        self.steps_taken += 1\n",
    "        self.previous_position = self.position\n",
    "        new_position = self.action_space[action]\n",
    "        \n",
    "        # Handle position changes\n",
    "        portfolio_reward = 0\n",
    "        if new_position != 0 and self.previous_position == 0:\n",
    "            self.last_position_change = self.current_step\n",
    "            self.position_start_value = self.portfolio_value\n",
    "        elif new_position == 0 and self.previous_position != 0:\n",
    "            self.last_position_change = None\n",
    "            self.position_start_value = None\n",
    "            portfolio_reward = self._calculate_portfolio_reward()\n",
    "\n",
    "        self.position = new_position\n",
    "        \n",
    "        # Calculate rewards\n",
    "        spread_return = self._calculate_immediate_reward()\n",
    "        baseline_action = self._get_baseline_action()\n",
    "        deviation_penalty = self.beta * abs(new_position - baseline_action)\n",
    "        self.reward = spread_return - deviation_penalty\n",
    "\n",
    "        # Update portfolio value\n",
    "        self.return_list.append(1 + spread_return)\n",
    "        self.cumulative_return = np.cumprod(self.return_list) -1\n",
    "        self.portfolio_value = self.cumulative_return[-1]\n",
    "        \n",
    "        # Store action\n",
    "        self.action_history.append({\n",
    "            'step': self.current_step,\n",
    "            'position': self.position,\n",
    "            'reward': self.reward,\n",
    "            'spread_return': spread_return,\n",
    "            'portfolio_value': self.portfolio_value,\n",
    "            'cumulative_return': self.cumulative_return\n",
    "        })\n",
    "        \n",
    "        # Advance step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Episode ends at end of data\n",
    "        done = self.current_step >= len(self.spread) - 1\n",
    "        \n",
    "        # Close position at end of episode\n",
    "        if done and self.position != 0:\n",
    "            self.position = 0\n",
    "            portfolio_reward = self._calculate_portfolio_reward()\n",
    "            self.reward += portfolio_reward\n",
    "        \n",
    "        return self._get_normalized_state(), self.reward, done, {\n",
    "            'portfolio_value': self.portfolio_value,\n",
    "            'spread_return': spread_return,\n",
    "            'deviation_penalty': deviation_penalty\n",
    "        }\n",
    "\n",
    "    def _get_normalized_state(self):\n",
    "        \"\"\"Get normalized state representation\"\"\"\n",
    "        spread = self.spread.iloc[self.current_step]\n",
    "        z_score = self._calculate_z_score(spread)\n",
    "        \n",
    "        return np.array([\n",
    "            self.position, \n",
    "            spread,\n",
    "            z_score\n",
    "        ])\n",
    "\n",
    "\n",
    "    def _calculate_z_score(self, spread):\n",
    "        if self.current_step < self.window_size:\n",
    "            return 0  \n",
    "        window = self.spread.iloc[self.current_step - self.window_size:self.current_step]\n",
    "        std_dev = window.std()\n",
    "        if std_dev == 0 or np.isnan(std_dev):\n",
    "            return 0  \n",
    "        return (spread - window.mean()) \/ std_dev\n",
    "\n",
    "    def _calculate_portfolio_reward(self):\n",
    "        if self.position_start_value is None:\n",
    "            return 0\n",
    "        return (self.portfolio_value - self.position_start_value) \/ self.position_start_value\n",
    "\n",
    "    def _calculate_immediate_reward(self):\n",
    "        asset1_return = self.asset_returns_raw[f'{self.asset1} Return'].iloc[self.current_step]\n",
    "        asset2_return = self.asset_returns_raw[f'{self.asset2} Return'].iloc[self.current_step]\n",
    "        prev_index = self.asset_returns_raw.index[self.current_step-1]\n",
    "        w2_prev = self.w2.loc[prev_index]\n",
    "        \n",
    "        spread_return = asset1_return - w2_prev * asset2_return\n",
    "\n",
    "        return self.previous_position * spread_return\n",
    "    \n",
    "    def _get_baseline_action(self):\n",
    "        z_score = self._calculate_z_score(self.spread.iloc[self.current_step])\n",
    "        lower_threshold = -1 * self.upper_threshold\n",
    "        \n",
    "        if z_score > self.upper_threshold and self.position < 0:  # short signal\n",
    "            return -1\n",
    "        elif z_score < lower_threshold and self.position > 0:  # long signal\n",
    "            return 1\n",
    "        elif abs(z_score) < self.close_threshold and self.position == 0:  # neutral\n",
    "            return 0\n",
    "        \n",
    "        return self.position  \n",
    "    "
   ],
   "execution_count":25,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"wRhppbYI0DdPVGNLwwiDYz",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def train_single_pair_dqn(env, episodes, hidden_size, batch_size=32):\n",
    "    state_size = 3  # position, spread, z-score\n",
    "    action_size = len(env.action_space)\n",
    "    \n",
    "    agent = DQNAgent(state_size, action_size, hidden_size)\n",
    "    \n",
    "    # Initialize training metrics\n",
    "    training_metrics = {\n",
    "        'best_model_positions': [],\n",
    "        'best_model_z_score': [],\n",
    "        'best_portfolio_value': float('-inf'),\n",
    "        'best_model_rmse': float('-inf'),\n",
    "        'best_reward': float('-inf'),\n",
    "        'best_cumulative_return': [],\n",
    "        'best_state_dict': None,\n",
    "        'training_rmse_history': [],  # Track RMSE across episodes\n",
    "        'reward_history': []  # Track rewards across episodes\n",
    "    }\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        episode_training_rmse = []\n",
    "        agent.reset_test_history()\n",
    "        positions = []\n",
    "        z_score = []\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            positions.append(next_state[0])\n",
    "            z_score.append(next_state[2])\n",
    "            \n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            if len(agent.memory) > batch_size:\n",
    "                training_rmse = agent.learn()\n",
    "                if training_rmse is not None:\n",
    "                    episode_training_rmse.append(training_rmse)\n",
    "\n",
    "            agent.actual_returns_history.append(reward)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        # Store episode metrics\n",
    "        training_metrics['reward_history'].append(episode_reward)\n",
    "        avg_rmse = np.mean(episode_training_rmse) if episode_training_rmse else float('inf')\n",
    "        training_metrics['training_rmse_history'].append(avg_rmse)\n",
    "        \n",
    "        # Update best model if performance improves\n",
    "        if episode_reward > training_metrics['best_reward']:\n",
    "            training_metrics['best_portfolio_value'] = env.portfolio_value\n",
    "            training_metrics['best_reward'] = episode_reward\n",
    "            training_metrics['best_state_dict'] = agent.policy_net.state_dict()\n",
    "            training_metrics['best_cumulative_return'] = env.cumulative_return\n",
    "            training_metrics['best_model_rmse'] = avg_rmse\n",
    "            training_metrics['best_model_positions'] = positions\n",
    "            training_metrics['best_model_z_score'] = z_score\n",
    "\n",
    "            # Update target network with best model\n",
    "            agent.target_net.load_state_dict(training_metrics['best_state_dict'])\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "    # Set the final network to the best found\n",
    "    agent.policy_net.load_state_dict(training_metrics['best_state_dict'])\n",
    "    agent.target_net.load_state_dict(training_metrics['best_state_dict'])\n",
    "    \n",
    "    return agent, training_metrics\n",
    "\n",
    "def backtest(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    agent.reset_test_history()\n",
    "    test_positions = []\n",
    "    test_z_score = []\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        test_positions.append(next_state[0])\n",
    "        test_z_score.append(next_state[2])\n",
    "        agent.actual_returns_history.append(reward)\n",
    "\n",
    "        if next_state is not None:\n",
    "            agent.next_states.append(next_state)\n",
    "        else:\n",
    "            agent.next_states.append(0)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    testing_rmse = agent.calculate_testing_rmse()\n",
    "\n",
    "    backtest_result = {\n",
    "        'action_history': env.action_history,\n",
    "        'testing_rmse': testing_rmse,\n",
    "        'testing_positions': test_positions,\n",
    "        'testing_z_score': test_z_score\n",
    "    }\n",
    "\n",
    "    return backtest_result"
   ],
   "execution_count":26,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"cjEYx1zmudnros7uHxGAHx",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from scipy.optimize import newton\n",
    "\n",
    "def calculate_yield(price, coupon, maturity_years=10, frequency=2, face_value=100):\n",
    "    \"\"\"Calculate yield using Newton's method - handles single values\"\"\"\n",
    "    if isinstance(coupon, pd.Series):\n",
    "        coupon = float(coupon.iloc[0])  # Take first value if Series\n",
    "        \n",
    "    def bond_price_diff(ytm):\n",
    "        \"\"\"Calculate difference between actual price and theoretical price at given YTM\"\"\"\n",
    "        periods = int(maturity_years * frequency)\n",
    "        coupon_payment = (coupon\/100\/frequency) * face_value  # Convert coupon to decimal\n",
    "        \n",
    "        # Sum of PV of all coupon payments\n",
    "        pv_coupons = sum(coupon_payment \/ (1 + ytm\/frequency)**(t+1) \n",
    "                        for t in range(periods))\n",
    "        \n",
    "        # PV of principal\n",
    "        pv_principal = face_value \/ (1 + ytm\/frequency)**periods\n",
    "        \n",
    "        return pv_coupons + pv_principal - price\n",
    "    \n",
    "    try:\n",
    "\n",
    "        initial_guess = (coupon\/100) * (face_value\/price)\n",
    "        \n",
    "        ytm = newton(bond_price_diff, initial_guess, \n",
    "                    tol=1e-7, maxiter=100)\n",
    "        return ytm\n",
    "        \n",
    "    except:\n",
    "        # Fallback if solver fails\n",
    "        print(f\"Solver failed for price={price}, coupon={coupon}\")\n",
    "        return (coupon\/100) \n",
    "\n",
    "def calculate_price(ytm, coupon, maturity_years=10, frequency=2, face_value=100):\n",
    "    \"\"\"Calculate bond price given yield - handles single values\"\"\"\n",
    "    if isinstance(coupon, pd.Series):\n",
    "        coupon = float(coupon.iloc[0])  # Take first value if Series\n",
    "        \n",
    "    periods = int(maturity_years * frequency)\n",
    "    coupon_payment = coupon\/frequency\n",
    "    pv = sum(coupon_payment \/ (1 + ytm\/frequency)**(t+1) for t in range(periods))\n",
    "    \n",
    "    pv += face_value \/ (1 + ytm\/frequency)**periods\n",
    "    return pv"
   ],
   "execution_count":27,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"1knVHa7ZnjEiuJIwlpHfrz",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "def generate_and_concatenate_shocked_returns_bonds(df, asset1, asset2, shock_bps, maturity_years=10):\n",
    "    asset1 = asset1.replace(' Adjusted','')\n",
    "    asset2 = asset2.replace(' Adjusted','')\n",
    "    bonds = [asset1, asset2]\n",
    "    \n",
    "    # Create price and coupon DataFrames for selected bonds\n",
    "    price_data = pd.DataFrame()\n",
    "    coupon_data = pd.DataFrame()\n",
    "    \n",
    "    for bond in bonds:\n",
    "        price_col = f\"{bond} Adjusted\"\n",
    "        coupon_col = f\"{bond} CPN\"\n",
    "        price_data[bond] = df[price_col]\n",
    "        coupon_data[bond] = df[coupon_col]\n",
    "    \n",
    "    # Calculate original returns\n",
    "    original_returns = price_data.pct_change()\n",
    "    original_returns.columns = [f\"{col} Adjusted Return\" for col in original_returns.columns]\n",
    "    \n",
    "    # List to store all return series\n",
    "    all_returns = [original_returns]\n",
    "    \n",
    "    # Generate date ranges for shocked series\n",
    "    date_increment = pd.Timedelta(days=1)\n",
    "    base_dates = df.index\n",
    "    current_start = base_dates[-1] + date_increment\n",
    "    \n",
    "    # Convert basis points to decimal\n",
    "    shock_levels = [x\/10000 for x in shock_bps]\n",
    "    \n",
    "    # Generate shocked returns with extended dates\n",
    "    for i, shock in enumerate(shock_levels):\n",
    "        # Create new dates for this shock period\n",
    "        shock_dates = pd.date_range(\n",
    "            start=current_start,\n",
    "            periods=len(base_dates),\n",
    "            freq=pd.infer_freq(base_dates)\n",
    "        )\n",
    "        current_start = shock_dates[-1] + date_increment\n",
    "\n",
    "        shocked_prices = pd.DataFrame(index=shock_dates)\n",
    "        \n",
    "        for bond in bonds:\n",
    "            # Calculate yields and shocked prices\n",
    "            yields = price_data[bond].apply(\n",
    "                lambda x: calculate_yield(float(x), float(coupon_data[bond].iloc[0]), maturity_years))\n",
    "\n",
    "            shocked_yields = yields + shock\n",
    "            \n",
    "            new_prices = shocked_yields.apply(\n",
    "                lambda x: calculate_price(float(x), float(coupon_data[bond].iloc[0]), maturity_years))\n",
    "            \n",
    "            shocked_prices[f\"{bond} Adjusted Return\"] = pd.Series(\n",
    "                data=new_prices.values,\n",
    "                index=shock_dates\n",
    "            )\n",
    "            \n",
    "        # Calculate returns and add data_type\n",
    "        shocked_returns = shocked_prices.pct_change()\n",
    "        \n",
    "        # Store shocked returns\n",
    "        all_returns.append(shocked_returns)\n",
    "        \n",
    "        print(f\"Shock {int(shock*10000)}bps - Length: {len(shocked_returns)}\")\n",
    "    \n",
    "    # Concatenate all return series\n",
    "    concatenated_returns = pd.concat(all_returns)\n",
    "    concatenated_returns = concatenated_returns.dropna()\n",
    "    \n",
    "    \n",
    "    return concatenated_returns"
   ],
   "execution_count":28,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"LL4CcV7Too1C9FkPxXRyK7",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def generate_and_concatenate_shocked_returns(original_price, vol_shocks=[0.5, 0.75, 1.0]):\n",
    "    original_dates = original_price.index\n",
    "    original_returns = original_price.pct_change()\n",
    "\n",
    "    original_cols = original_returns.columns\n",
    "    renamed_cols = [f\"{col} Return\" for col in original_cols]\n",
    "    original_returns.columns = renamed_cols\n",
    "\n",
    "    # List to store all return series\n",
    "    all_returns = [original_returns]\n",
    "    std_prices = original_price.std()\n",
    "    last_date = original_dates[-1]\n",
    "    \n",
    "    for i, vol_shock in enumerate(vol_shocks, 1):\n",
    "        # Create shocked prices with extended dates\n",
    "        new_dates = pd.date_range(start=last_date + pd.Timedelta(days=1),\n",
    "                                periods=len(original_dates),\n",
    "                                freq=pd.infer_freq(original_dates))\n",
    "        \n",
    "        shocked_prices = pd.DataFrame(index=new_dates)\n",
    "        for col in original_price.columns:\n",
    "            shocked_prices[f\"{col} Return\"] = original_price[col].values + vol_shock * std_prices[col]\n",
    "        \n",
    "        shocked_returns = shocked_prices.pct_change()\n",
    "        all_returns.append(shocked_returns)\n",
    "        last_date = new_dates[-1]\n",
    "    concatenated_returns = pd.concat(all_returns)\n",
    "    concatenated_returns = concatenated_returns.dropna()\n",
    "    \n",
    "    return concatenated_returns\n",
    "\n",
    "def train_single_pair_dqn_concatenated(concatenated_returns, asset1, asset2, window_size, upper_threshold, \n",
    "                                     close_threshold, beta, hidden_size, episodes=70):\n",
    "    \"\"\"\n",
    "    Train agent on concatenated returns data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create environment with concatenated returns\n",
    "    env = SinglePairTradingEnv(concatenated_returns, asset1, asset2, \n",
    "                              upper_threshold, close_threshold, window_size, beta)\n",
    "    \n",
    "    \n",
    "    # Initialize agent and metrics\n",
    "    state_size = 3\n",
    "    action_size = 3  # -1, 0, 1\n",
    "    agent = DQNAgent(state_size, action_size, hidden_size)\n",
    "    \n",
    "    training_metrics = {\n",
    "        'best_model_positions': [],\n",
    "        'best_model_z_score': [],\n",
    "        'best_portfolio_value': float('-inf'),\n",
    "        'best_model_rmse': float('inf'),\n",
    "        'best_reward': float('-inf'),\n",
    "        'best_cumulative_return': [],\n",
    "        'best_state_dict': None,\n",
    "        'training_rmse_history': [],\n",
    "        'reward_history': []\n",
    "    }\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        episode_rmse = []\n",
    "        current_positions = []\n",
    "        current_z_scores = []\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Learn if enough samples\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                rmse = agent.learn()\n",
    "                if rmse is not None:\n",
    "                    episode_rmse.append(rmse)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            current_positions.append(next_state[0])\n",
    "            current_z_scores.append(next_state[2])\n",
    "            \n",
    "            state = next_state\n",
    "        if episode % 10 ==0:\n",
    "            print('episode',episode)\n",
    "        \n",
    "        # Store episode metrics\n",
    "        training_metrics['reward_history'].append(episode_reward)\n",
    "        avg_rmse = np.mean(episode_rmse) if episode_rmse else float('inf')\n",
    "        training_metrics['training_rmse_history'].append(avg_rmse)\n",
    "        \n",
    "        # Update best model if performance improves\n",
    "        # if avg_rmse < training_metrics['best_model_rmse']:\n",
    "        if episode_reward > training_metrics['best_reward']:\n",
    "            training_metrics['best_reward'] = episode_reward\n",
    "            training_metrics['best_state_dict'] = agent.policy_net.state_dict()\n",
    "            training_metrics['best_model_rmse'] = avg_rmse\n",
    "            training_metrics['best_model_positions'] = current_positions\n",
    "            training_metrics['best_model_z_score'] = current_z_scores\n",
    "            training_metrics['best_cumulative_return'] = env.cumulative_return\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "    \n",
    "    # Set final network to best found\n",
    "    agent.policy_net.load_state_dict(training_metrics['best_state_dict'])\n",
    "    agent.target_net.load_state_dict(training_metrics['best_state_dict'])\n",
    "    \n",
    "    return agent, training_metrics"
   ],
   "execution_count":29,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"RrdMEETDZqdg0U9m6G1jMG",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def plot_cumulative_returns(dates, training_returns, type):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dates, training_returns, label='Returns')\n",
    "    plt.title(f'Cumulative Returns in {type} set')\n",
    "    plt.xlabel('Dates')\n",
    "    plt.ylabel('Cumulative Return')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_comparative_trading_behavior(dates, action_results, rule_based_result, upper_threshold, data_type, hidden_size, window_size):\n",
    "\n",
    "    hidden_size_str = '-'.join(str(layer) for layer in hidden_size)\n",
    "    if data_type == 'Testing':\n",
    "        dqn_positions = action_results['testing_positions']\n",
    "        z_score_history = action_results['testing_z_score']\n",
    "\n",
    "    elif data_type == 'Training':\n",
    "        dqn_positions = action_results['best_model_positions']\n",
    "        z_score_history = action_results['best_model_z_score']\n",
    "\n",
    "    rule_positions = rule_based_result['positions'][window_size+1:]\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = plt.GridSpec(2, 1, height_ratios=[3, 2])\n",
    "\n",
    "    # First subplot: DQN strategy\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax1.plot(dates, z_score_history, color='blue', linewidth=0.8, label='Spread')\n",
    "    ax1.plot(dates, [-upper_threshold] * len(z_score_history), 'g--', linewidth=0.8, label='Buy Threshold')\n",
    "    ax1.plot(dates, [upper_threshold] * len(z_score_history), 'r--', linewidth=0.8, label='Sell Threshold')\n",
    "    \n",
    "    # Plot DQN signals\n",
    "    buy_dates = dates[dqn_positions == 1]\n",
    "    sell_dates = dates[dqn_positions == -1]\n",
    "    \n",
    "    for i in range(len(dates)):\n",
    "        if dqn_positions[i] == 1:\n",
    "            ax1.scatter(dates[i], z_score_history[i], color='green', marker='^', s=50)\n",
    "        elif dqn_positions[i] == -1:\n",
    "            ax1.scatter(dates[i], z_score_history[i], color='red', marker='v', s=50)\n",
    "    \n",
    "    ax1.set_title(f'DQN Trading Signals - {data_type} for {hidden_size_str}')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rule-based Strategy subplot\n",
    "    ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "    ax2.plot(dates, z_score_history, color='blue', linewidth=0.8, label='Spread')\n",
    "    ax2.plot(dates, [-upper_threshold] * len(z_score_history), 'g--', linewidth=0.8, label='Buy Threshold')\n",
    "    ax2.plot(dates, [upper_threshold] * len(z_score_history), 'r--', linewidth=0.8, label='Sell Threshold')\n",
    "    \n",
    "    # Plot rule-based signals\n",
    "    for i in range(len(dates)):\n",
    "        if rule_positions[i] == 1:\n",
    "            ax2.scatter(dates[i], z_score_history[i], color='green', marker='^', s=50)\n",
    "        elif rule_positions[i] == -1:\n",
    "            ax2.scatter(dates[i], z_score_history[i], color='red', marker='v', s=50)\n",
    "    \n",
    "    ax2.set_title(f'Rule-based Trading Signals {data_type}-{hidden_size}')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    \n",
    "    # # Create difference highlights\n",
    "    difference_indices = [i for i in range(len(dates)) if rule_positions[i] != dqn_positions[i]]\n",
    "    difference_dates = [dates[i] for i in difference_indices]\n",
    "    difference_scores = [z_score_history[i] for i in difference_indices]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Print summary statistics\n",
    "    total_differences = len(difference_indices)\n",
    "    total_periods = len(dates)\n",
    "    percentage_difference = total_differences\/total_periods\n",
    "\n",
    "    return total_differences, percentage_difference\n",
    "    \n",
    "\n",
    "\n",
    "def get_performance_metrics(dates, action_results, upper_threshold, rule_based_results, data_type, hidden_size, window_size):\n",
    "    if data_type == 'Testing':\n",
    "        action_history = action_results['action_history']\n",
    "        rmse = action_results['testing_rmse']     \n",
    "        last_action = action_history[-1]\n",
    "        cumulative_return = last_action['cumulative_return']\n",
    "    elif data_type == 'Training':\n",
    "        rmse = action_results['best_model_rmse']\n",
    "        cumulative_return = action_results['best_cumulative_return']\n",
    "\n",
    "    # Plot performance visualizations\n",
    "    total_differences, percentage_difference = plot_comparative_trading_behavior(dates, action_results, rule_based_results, upper_threshold, data_type, hidden_size, window_size)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    mean_return, sharpe = get_summary(cumulative_return)\n",
    "    rule_based_cum_return = rule_based_results['cum_return']\n",
    "    rule_based_mean_return, rule_based_sharpe = get_summary(rule_based_cum_return)\n",
    "    \n",
    "    dqn_strategy_summary = {\n",
    "        'Data Type': data_type,\n",
    "        # 'Mean Return (%) Annual': \"{:.4f}\".format(mean_return),\n",
    "        'Sharpe Ratio Annual': \"{:.4f}\".format(sharpe),\n",
    "        'RMSE': \"{:.4f}\".format(rmse),\n",
    "        'Hidden Size':'-'.join(str(layer) for layer in hidden_size),\n",
    "        'Total Difference': total_differences,\n",
    "        'Percentage Difference': \"{:.2f}%\".format(percentage_difference * 100)\n",
    "    }\n",
    "\n",
    "    rule_based_summary = {\n",
    "        'Data Type': f'Naive Strat -{data_type}',\n",
    "        # 'Mean Return (%) Annual': \"{:.4f}\".format(rule_based_mean_return),\n",
    "        'Sharpe Ratio Annual': \"{:.4f}\".format(rule_based_sharpe),\n",
    "        'RMSE': 0,\n",
    "        'Hidden Size':'-'.join(str(layer) for layer in hidden_size),\n",
    "        'Total Difference': 0,\n",
    "        'Percentage Difference': 0\n",
    "    }\n",
    "    return dqn_strategy_summary, rule_based_summary\n",
    "\n",
    "def get_summary(cumulative_return):\n",
    "    periodic_returns = (1 + np.array(cumulative_return[1:])) \/ (1 + np.array(cumulative_return[:-1])) - 1\n",
    "    periodic_returns = periodic_returns[~np.isinf(periodic_returns) & ~np.isneginf(periodic_returns)]\n",
    "    periodic_returns = periodic_returns[~np.isnan(periodic_returns)]\n",
    "    \n",
    "    # Mean return\n",
    "    mean_return = np.mean(periodic_returns) * 252 * 100\n",
    "    \n",
    "    # Sharpe ratio (assuming risk-free rate = 0)\n",
    "    sharpe = (np.mean(periodic_returns) \/ np.std(periodic_returns)) * np.sqrt(252)\n",
    "    \n",
    "    return mean_return, sharpe"
   ],
   "execution_count":30,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"UnQLGJPYHFjp1iGgkzUbhP",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "if __name__ == \"__main__\":\n",
    "    # Setup parameters\n",
    "    start_year = 2014\n",
    "    end_year = 2024\n",
    "    summary_data = []\n",
    "    \n",
    "    training_testing_Split = '2018-12-31'\n",
    "    train_period = tr_dm_net[tr_dm_net.index <= training_testing_Split]\n",
    "    test_period = tr_dm_net[tr_dm_net.index > training_testing_Split]\n",
    "    train_period_price = tr_dm_net_price[tr_dm_net_price.index <= training_testing_Split]\n",
    "    test_period_price = tr_dm_net_price[tr_dm_net_price.index > training_testing_Split]\n",
    "    train_tr_dm_for_shock = tr_dm_for_shock[tr_dm_for_shock.index <= training_testing_Split]\n",
    "\n",
    "    asset1 = 'GTSEK10Y Govt Adjusted'\n",
    "    asset2 = 'GTCHF10Y Govt Adjusted'\n",
    "    # concatenated_returns = generate_and_concatenate_shocked_returns(train_period_price)\n",
    "    concatenated_returns = generate_and_concatenate_shocked_returns_bonds(train_tr_dm_for_shock, asset1, asset2, shock_bps=[100, 200, -100])\n",
    "    window_size = 60\n",
    "    upper_threshold = 1.0\n",
    "    close_threshold = 0.1\n",
    "    beta = 0.8\n",
    "\n",
    "    rule_based_train_result = rule_based_strategy(concatenated_returns, window_size, upper_threshold,close_threshold, asset1,asset2)\n",
    "    rule_based_test_result = rule_based_strategy(test_period, window_size, upper_threshold,close_threshold, asset1,asset2)\n",
    "    # Define network architectures\n",
    "    hidden_size_list = [[64,32],[32,16],[5,2],[128,128]]\n",
    "    \n",
    "    for hidden_size in hidden_size_list:\n",
    "        print(f\"\\nTraining with hidden size: {hidden_size}\")\n",
    "        \n",
    "        # Train with synthetic data\n",
    "        best_agent, train_metrics = train_single_pair_dqn_concatenated(\n",
    "            concatenated_returns,\n",
    "            asset1=asset1,\n",
    "            asset2=asset2,\n",
    "            window_size=window_size,\n",
    "            upper_threshold=upper_threshold,\n",
    "            close_threshold=close_threshold,\n",
    "            beta=beta,\n",
    "            hidden_size=hidden_size,\n",
    "            episodes=150\n",
    "        )\n",
    "\n",
    "        # Training dates\n",
    "        training_dates = concatenated_returns.index[window_size+1:]\n",
    "\n",
    "        train_dqn_strategy_summary, rule_based_summary_train = get_performance_metrics(training_dates, train_metrics, upper_threshold, rule_based_train_result, 'Training', hidden_size, window_size)\n",
    "        \n",
    "        rule_based_summary_train['Pair'] = f\"{asset1} - {asset2}\"\n",
    "        train_dqn_strategy_summary['Pair'] = f\"{asset1} - {asset2}\"\n",
    "\n",
    "        rule_based_summary_train['Beta'] = beta\n",
    "        train_dqn_strategy_summary['Beta'] = beta\n",
    "\n",
    "        summary_data.append(rule_based_summary_train)\n",
    "        summary_data.append(train_dqn_strategy_summary)\n",
    "\n",
    "\n",
    "\n",
    "        # Test best agent\n",
    "        env_test = SinglePairTradingEnv(test_period, asset1, asset2, upper_threshold, close_threshold, window_size, beta)\n",
    "        \n",
    "        testing_dates = test_period.index[window_size+1:]\n",
    "        backtest_results = backtest(env_test, best_agent)\n",
    "        \n",
    "        # Get and store testing metrics\n",
    "        test_dqn_strategy_summary, rule_based_summary_test = get_performance_metrics(testing_dates, backtest_results, upper_threshold, rule_based_test_result, 'Testing', hidden_size, window_size)\n",
    "        rule_based_summary_test['Pair'] = f\"{asset1} - {asset2}\"\n",
    "        test_dqn_strategy_summary['Pair'] = f\"{asset1} - {asset2}\"\n",
    "        rule_based_summary_test['Beta'] = beta\n",
    "        test_dqn_strategy_summary['Beta'] = beta\n",
    "        summary_data.append(test_dqn_strategy_summary)\n",
    "        summary_data.append(rule_based_summary_test)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    columns = ['Pair','Data Type','Hidden Size','Beta', 'Sharpe Ratio Annual', 'RMSE','Total Difference','Percentage Difference']\n",
    "    summary_df = summary_df[columns]"
   ],
   "execution_count":0,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Shock 100bps - Length: 1182\n",
      "Shock 200bps - Length: 1182\n",
      "Shock -100bps - Length: 1182\n",
      "\n",
      "Training with hidden size: [64, 32]\n",
      "episode 0\n",
      "episode 10\n",
      "episode 20\n",
      "episode 30\n",
      "episode 40\n",
      "episode 50\n",
      "episode 60\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"pCy9aCoFGXf7oFzi2Gi9rK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "pivot_df = pd.pivot_table(\n",
    "   summary_df,\n",
    "   index=['Hidden Size', 'Pair','Beta'],\n",
    "   columns='Data Type',\n",
    "   values=['Sharpe Ratio Annual','RMSE','Total Difference','Percentage Difference'],\n",
    "   aggfunc='first' \n",
    ").round(4)\n",
    "pivot_df.head(60)"
   ],
   "execution_count":null,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th colspan=\"4\" halign=\"left\">Percentage Difference<\/th>\n",
       "      <th colspan=\"4\" halign=\"left\">RMSE<\/th>\n",
       "      <th colspan=\"4\" halign=\"left\">Sharpe Ratio Annual<\/th>\n",
       "      <th colspan=\"4\" halign=\"left\">Total Difference<\/th>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th>Data Type<\/th>\n",
       "      <th>Naive Strat -Testing<\/th>\n",
       "      <th>Naive Strat -Training<\/th>\n",
       "      <th>Testing<\/th>\n",
       "      <th>Training<\/th>\n",
       "      <th>Naive Strat -Testing<\/th>\n",
       "      <th>Naive Strat -Training<\/th>\n",
       "      <th>Testing<\/th>\n",
       "      <th>Training<\/th>\n",
       "      <th>Naive Strat -Testing<\/th>\n",
       "      <th>Naive Strat -Training<\/th>\n",
       "      <th>Testing<\/th>\n",
       "      <th>Training<\/th>\n",
       "      <th>Naive Strat -Testing<\/th>\n",
       "      <th>Naive Strat -Training<\/th>\n",
       "      <th>Testing<\/th>\n",
       "      <th>Training<\/th>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>Hidden Size<\/th>\n",
       "      <th>Pair<\/th>\n",
       "      <th>Beta<\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128-128<\/th>\n",
       "      <th>GTSEK10Y Govt Adjusted - GTDEM10Y Govt Adjusted<\/th>\n",
       "      <th>0.8<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>40.36%<\/td>\n",
       "      <td>65.22%<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0104<\/td>\n",
       "      <td>0.0765<\/td>\n",
       "      <td>0.3206<\/td>\n",
       "      <td>0.4253<\/td>\n",
       "      <td>-1.0631<\/td>\n",
       "      <td>0.5228<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>576<\/td>\n",
       "      <td>3041<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>32-16<\/th>\n",
       "      <th>GTSEK10Y Govt Adjusted - GTDEM10Y Govt Adjusted<\/th>\n",
       "      <th>0.8<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>79.19%<\/td>\n",
       "      <td>68.56%<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0.1499<\/td>\n",
       "      <td>0.1008<\/td>\n",
       "      <td>0.3206<\/td>\n",
       "      <td>0.4253<\/td>\n",
       "      <td>0.2704<\/td>\n",
       "      <td>0.4992<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1130<\/td>\n",
       "      <td>3197<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>5-2<\/th>\n",
       "      <th>GTSEK10Y Govt Adjusted - GTDEM10Y Govt Adjusted<\/th>\n",
       "      <th>0.8<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>80.66%<\/td>\n",
       "      <td>68.30%<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0109<\/td>\n",
       "      <td>0.1268<\/td>\n",
       "      <td>0.3206<\/td>\n",
       "      <td>0.4253<\/td>\n",
       "      <td>0.6487<\/td>\n",
       "      <td>0.4577<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1151<\/td>\n",
       "      <td>3185<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>64-32<\/th>\n",
       "      <th>GTSEK10Y Govt Adjusted - GTDEM10Y Govt Adjusted<\/th>\n",
       "      <th>0.8<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>79.68%<\/td>\n",
       "      <td>69.91%<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0269<\/td>\n",
       "      <td>0.1228<\/td>\n",
       "      <td>0.3206<\/td>\n",
       "      <td>0.4253<\/td>\n",
       "      <td>0.2222<\/td>\n",
       "      <td>0.3013<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1137<\/td>\n",
       "      <td>3260<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ntnd2Z5cgaaaZpdYETBqK4",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Sheet 2"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Sheet 2",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Appendix: Coherent pair and noncoherent pair comparison"
   ],
   "outputs":[
    {
     "data":{
      "application\/datalore_markdown_expressions+json":{
       "expressions":{}
      }
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"IMxak23liryr8eOiwzrFQX",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Training\/testing data split\n",
    "training_testing_Split = '2018-12-31'\n",
    "training_return = tr_dm_net[tr_dm_net.index <= training_testing_Split]\n",
    "testing_return = tr_dm_net[tr_dm_net.index > training_testing_Split]\n",
    "\n",
    "training_price = tr_dm_net_price[tr_dm_net_price.index <= training_testing_Split]\n",
    "testing_price = tr_dm_net_price[tr_dm_net_price.index > training_testing_Split]\n"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"rlezeS29ZVpUIjXxUIi306",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "def evaluate_pair_performance(pairs_list, data, window_size, upper_threshold, close_threshold):\n",
    "    pairs_metrics = {}\n",
    "    \n",
    "    for asset1, asset2 in pairs_list:\n",
    "\n",
    "        z_score, spread, returns, cumulative_return, positions = rule_based_strategy_info(data, window_size, upper_threshold, close_threshold, asset1, asset2)\n",
    "\n",
    "        metrics = {\n",
    "            'Cumulative Return': cumulative_return[-1],\n",
    "            # 'Max Drawdown': calculate_max_drawdown(returns),\n",
    "            'Spread Volatility': spread.std(),\n",
    "            'signal_count': len(z_score[abs(z_score) > upper_threshold]),\n",
    "            'avg_deviation': abs(z_score).mean(),\n",
    "            'max_deviation': abs(z_score).max(),\n",
    "            'signal_frequency': len(z_score[abs(z_score) > upper_threshold]) \/ len(z_score),\n",
    "            'Average Trade Return': np.mean(returns[returns != 0])\n",
    "        }\n",
    "\n",
    "        pairs_metrics[(asset1.replace(\"Adjusted Return\",\"\"), asset2.replace(\"Adjusted Return\",\"\"))] = metrics\n",
    "            \n",
    "    return pairs_metrics\n",
    "\n",
    "def compare_coherent_noncoherent(coherent_pair, noncoherent_pair, data, \n",
    "                                 window_size_coherent, upper_threshold_coherent, close_threshold_coherent,\n",
    "                                 window_size_noncoherent, upper_threshold_noncoherent, close_threshold_noncoherent):\n",
    "\n",
    "    coherent_metrics = evaluate_pair_performance(coherent_pair, data, \n",
    "                                               window_size_coherent, upper_threshold_coherent, close_threshold_coherent)\n",
    "    \n",
    "    noncoherent_metrics = evaluate_pair_performance(noncoherent_pair, data, \n",
    "                                                window_size_noncoherent, upper_threshold_noncoherent, close_threshold_noncoherent)\n",
    "    \n",
    "    \n",
    "    return coherent_metrics, noncoherent_metrics\n"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"U6rHqOMmm33eTToJDfdJHA",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "window_size_list = [30, 60, 90]\n",
    "upper_threshold_list = [1.0, 1.25, 1.5]\n",
    "close_threshold_list = [0.1, 0.5, 1]\n",
    "\n",
    "coherent_pair, noncoherent_pair = noncoherent_pair_cluster(training_return, training_price)\n",
    "# upper_threshold_coherent=upper_threshold_noncoherent = 1.0\n",
    "# close_threshold_coherent = close_threshold_noncoherent = 0.1\n",
    "# window_size_coherent = window_size_noncoherent = 60\n",
    "\n",
    "\n",
    "upper_threshold_coherent, close_threshold_coherent, window_size_coherent = optimize_parameters(training_return, coherent_pair, window_size_list, upper_threshold_list, close_threshold_list)\n",
    "upper_threshold_noncoherent, close_threshold_noncoherent, window_size_noncoherent = optimize_parameters(training_return, noncoherent_pair, window_size_list, upper_threshold_list, close_threshold_list)\n",
    "\n",
    "coherent_metrics, noncoherent_metrics= compare_coherent_noncoherent(coherent_pair, noncoherent_pair,training_return, \n",
    "                                window_size_coherent, upper_threshold_coherent, close_threshold_coherent,\n",
    "                                window_size_noncoherent, upper_threshold_noncoherent, close_threshold_noncoherent)\n",
    "\n",
    "test_coherent_metrics, test_noncoherent_metrics= compare_coherent_noncoherent(coherent_pair, noncoherent_pair,testing_return, \n",
    "                                window_size_coherent, upper_threshold_coherent, close_threshold_coherent,\n",
    "                                window_size_noncoherent, upper_threshold_noncoherent, close_threshold_noncoherent)\n",
    "\n",
    "\n",
    "coherent_df = pd.DataFrame(coherent_metrics).round(4).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAIN COHERENT PAIRS PERFORMANCE\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(coherent_df.to_string())\n",
    "\n",
    "noncoherent_df = pd.DataFrame(noncoherent_metrics).round(4).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAIN NONCOHERENT PAIRS PERFORMANCE\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(noncoherent_df.to_string())\n",
    "\n",
    "\n",
    "test_coherent_df = pd.DataFrame(test_coherent_metrics).round(4).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST COHERENT PAIRS PERFORMANCE\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(test_coherent_df.to_string())\n",
    "\n",
    "test_noncoherent_df = pd.DataFrame(test_noncoherent_metrics).round(4).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST NONCOHERENT PAIRS PERFORMANCE\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(test_noncoherent_df.to_string())"
   ],
   "execution_count":null,
   "outputs":[
    {
     "ename":"KeyError",
     "evalue":"KeyError: 'cum_returns'",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 11 in <module>",
      "    at line 22 in optimize_parameters(data, noncoherent_pair, window_size_list, upper_threshold_list, close_threshold_list)",
      "KeyError: 'cum_returns'"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"IHHD3HGgCDxC9mzztO1Bxd",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Sheet 3"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Sheet 3",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def add_yield_shock_to_bonds(price_data, coupon_rate, maturity, shock_bps=100):\n",
    "    \"\"\"\n",
    "    Add yield shock to historical bond prices\n",
    "    \n",
    "    Parameters:\n",
    "    price_data: Series\/array of historical bond prices\n",
    "    coupon_rate: Annual coupon rate (decimal)\n",
    "    maturity: Years to maturity\n",
    "    shock_bps: Shock in basis points to add to yield\n",
    "    \"\"\"\n",
    "    def solve_for_ytm(price, guess=0.05):\n",
    "        \"\"\"Find yield to maturity using Newton's method\"\"\"\n",
    "        def npv(y):\n",
    "            periods = int(maturity * 2)  # Semi-annual payments\n",
    "            coupon = (coupon_rate\/2) * 1000  # Semi-annual coupon payment\n",
    "            r = y\/2  # Semi-annual yield\n",
    "            \n",
    "            # Calculate present value\n",
    "            pv_coupons = coupon * (1 - (1+r)**(-periods))\/r\n",
    "            pv_principal = 1000\/(1+r)**periods\n",
    "            return pv_coupons + pv_principal - price\n",
    "            \n",
    "        def npv_prime(y):\n",
    "            # Numerical approximation of derivative\n",
    "            delta = 1e-5\n",
    "            return (npv(y + delta) - npv(y))\/delta\n",
    "            \n",
    "        # Newton's method iteration\n",
    "        y = guess\n",
    "        for _ in range(100):\n",
    "            diff = npv(y)\/npv_prime(y)\n",
    "            y -= diff\n",
    "            if abs(diff) < 1e-7:\n",
    "                break\n",
    "        return y\n",
    "\n",
    "    shocked_prices = []\n",
    "    for price in price_data:\n",
    "        # Calculate original YTM\n",
    "        ytm = solve_for_ytm(price)\n",
    "        \n",
    "        # Add shock\n",
    "        shocked_ytm = ytm + (shock_bps\/10000)\n",
    "        \n",
    "        # Calculate new price\n",
    "        periods = int(maturity * 2)\n",
    "        r = shocked_ytm\/2\n",
    "        coupon = (coupon_rate\/2) * 1000\n",
    "        \n",
    "        pv_coupons = coupon * (1 - (1+r)**(-periods))\/r\n",
    "        pv_principal = 1000\/(1+r)**periods\n",
    "        shocked_price = pv_coupons + pv_principal\n",
    "        \n",
    "        shocked_prices.append(shocked_price)\n",
    "    \n",
    "    return pd.Series(shocked_prices, index=price_data.index)\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "historical_prices = pd.Series([...])  # Your historical price data\n",
    "coupon_rate = 0.04  # 4% annual coupon\n",
    "maturity = 10  # 10-year bond\n",
    "\n",
    "shocked_prices = add_yield_shock_to_bonds(\n",
    "    historical_prices, \n",
    "    coupon_rate=coupon_rate,\n",
    "    maturity=maturity,\n",
    "    shock_bps=100\n",
    ")\n",
    "\"\"\""
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"B9rhnEwzlEXrnbQGr7sD7H",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Sheet 4"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Sheet 4",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"YdE3NdrWGYzwJAs4erGxHN",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "\n",
    "Deep Q-Network for Statistical Arbitrage: A Reinforcement Learning Approach to Pair Trading\n",
    "\n",
    "Abstract\n",
    "This paper presents a novel approach to statistical arbitrage by implementing a Deep Q-Network (DQN) reinforcement learning algorithm for pair trading. Through the application of deep reinforcement learning techniques, we demonstrate how DQN can learn optimal trading decisions by considering both traditional statistical measures and dynamic market conditions. Our empirical results show that this approach achieves superior performance compared to conventional threshold-based methodologies, particularly in its ability to adapt to changing market conditions and manage risk effectively.\n",
    "\n",
    "1. Introduction\n",
    "1.1 Background\n",
    "Statistical arbitrage has long been a cornerstone strategy in quantitative trading, with pair trading representing one of its most well-known implementations. Traditional pair trading approaches rely on predetermined statistical thresholds to trigger trading signals, assuming that the spread between two cointegrated securities will maintain historical patterns. However, these conventional methods often fail to adapt to evolving market dynamics and may miss opportunities or incur losses when historical relationships temporarily break down. \n",
    "Reinforcement learning (RL) offers a natural solution to this limitation by learning optimal trading policies through direct interaction with the market environment. Unlike traditional methods, RL agents can generalize patterns and recognize subtle shifts in market regimes that might not be captured by simple statistical thresholds. Furthermore, RL's ability to optimize for long-term rewards rather than immediate profits allows it to develop more sophisticated trading strategies that balance risk and return across different market conditions. The adaptive nature of RL makes it particularly well-suited for pair trading, where the relationship between securities can evolve over time and traditional static rules may become suboptimal.\n",
    "\n",
    "1.2 Research Objectives\n",
    "This research introduces a novel framework that leverages deep Q-learning to overcome the limitations of traditional pair trading approaches. By implementing a Deep Q-Network architecture, we aim to develop a trading system that can dynamically adjust its strategy based on market conditions while maintaining the fundamental principles of statistical arbitrage. Our approach moves beyond simple threshold-based rules to incorporate a broader range of market indicators and position management considerations.\n",
    "\n",
    "2. Methodology\n",
    "2.1 Pair Selection Process\n",
    "The selection of proper trading pairs follows a three-step hierarchical approach that systematically identifies pairs with stable and tradeable relationships. This methodology combines principal component analysis, cointegration testing, and factor sensitivity analysis to ensure robust pair identification.\n",
    "The first step involves clustering bonds based on their loadings on the first principal component (PC1), which typically represents the dominant market-wide interest rate risk factor. The economic intuition behind this initial clustering is that PC1 captures the parallel shift in yield curves, also known as the level factor. By grouping bonds with similar PC1 loadings, we identify securities that respond similarly to broad market movements. The process begins with standardized returns to ensure the clustering is based on correlation patterns rather than being dominated by volatility differences. The optimal number of clusters is determined through the elbow method, providing a data-driven approach to group formation.\n",
    "Following the initial clustering, the second step examines cointegration relationships among pairs within each identified cluster. Cointegration testing is fundamental as it validates the statistical basis for mean-reversion trading strategies. This step is particularly crucial because it identifies pairs whose price spreads tend to maintain a long-term mean-reverting relationship despite short-term deviations. By testing for cointegration within clusters rather than across the entire universe, we focus on economically meaningful relationships while maintaining computational efficiency. The use of price levels rather than returns in this analysis helps capture persistent long-term relationships between securities.\n",
    "The third step introduces a refined analysis of secondary risk factors through examination of PC2. In our project, PC2 captures geographical proximity and regional economic linkages between sovereign bonds. This geographical interpretation of PC2 is particularly meaningful for our dataset of international sovereign bonds, as the pairs of bonds with similar PC2 are subject to similar regional macroeconomic conditions, monetary policies, and risk factors. For instance, bonds from countries within the Eurozone or Nordic regions tend to exhibit similar PC2 loadings, reflecting their economic interconnectedness and shared policy environments. The threshold for acceptable PC2 loading differences is set at the median of observed differences, providing a balanced approach to pair selection.\n",
    "To further investigate the properties of pairs of bonds with similar PC2 and those with divergent behavior on the second principal component, we compared these two groups from the aspects of \n",
    "\n",
    "The strength of this approach lies in its comprehensive consideration of both statistical and economic factors. By combining principal component analysis with cointegration testing, the methodology captures both the cross-sectional and time-series aspects of bond market relationships. The addition of secondary factor analysis provides an extra layer of scrutiny, helping to identify pairs that are likely to maintain their relationship across different market conditions. This thorough filtering process results in a select group of pairs that are well-suited for statistical arbitrage strategies, with reduced risk of relationship breakdown during periods of market stress.\n",
    "\n",
    "2.2 Rule-Based Statistical Arbitrage\n",
    "The rule-based statistical arbitrage strategy implements a systematic approach to pair trading through several key steps, incorporating parameter optimization through grid search to enhance strategy performance.\n",
    "First, the strategy employs rolling Principal Component Analysis (PCA) to identify the common trend between the two assets. The PCA loadings are calculated on a rolling basis to avoid look-ahead bias over time. The strategy focuses on the loadings for the second principal component (PC2), which reflects the bond-specific factors that are relatively insensitive (orthogonal) to broader market movements captured by PC1. The code extracts the rolling loadings of the selected assets on PC2 and uses these loadings to construct the spread.\n",
    "The spread is calculated as a long\/short linear combination of returns from two assets: Spread = w₁ * Asset₁ Return - w₂ * Asset₂ Return, where the weights w₁ is normalized to 1 and the second weight, w₂, is determined through a formula that incorporates ratio of rolling volatilities and the ratio of 2nd principal component loading: w₂ = w₁ * (σ₁\/σ₂) * (PC 2 Loading₁\/PC2 Loading₂).\n",
    "\n",
    "Trading signals are generated using a z-score approach that measures the deviation of the spread from its rolling mean in units of rolling standard deviation. The critical parameters that govern signal generation are determined through an exhaustive grid search process that tests multiple combinations of three key parameters: the lookback window size for calculating rolling statistics, the entry thresholds for position initiation, and the exit thresholds for position closure. Each parameter combination is evaluated across all trading pairs to identify the set that maximizes average cumulative returns while maintaining strategy robustness.\n",
    "Position management follows optimized rules based on the grid search results:\n",
    "1. When the z-score exceeds the optimized upper threshold, indicating the spread is significantly positive, we take a short position (-1) expecting mean reversion.\n",
    "2. When the z-score falls below the optimized lower threshold, indicating the spread is significantly negative, we take a long position (1).\n",
    "3. Positions are closed when the z-score reverts to within the optimized close_threshold bands, capturing the mean reversion profit.\n",
    "The strategy ensures logical parameter relationships are maintained, such as requiring the close threshold to be smaller than the entry threshold, which helps prevent illogical trading scenarios. The optimization process evaluates the performance across all pairs to find parameters that work well universally rather than being overfitted to specific pairs.\n",
    "The strategy calculates returns by combining the positions with the actual asset returns, considering the position sizes determined by w1 and w2. A one-period lag is applied to the positions to reflect realistic trading implementation where signals from one period generate trades for the next period. The cumulative performance is tracked through the cumulative product of the returns, providing a measure of the strategy's effectiveness over time.\n",
    "\n",
    "\n",
    "2.2 Reinforcement Learning Strategy\n",
    " \n",
    "The reinforcement learning based statistical arbitrage strategy leverages off-policy Deep Q-learning model to discover and exploit trading opportunities in pairs of cointegrated securities. Unlike traditional rule-based approaches that rely on fixed thresholds, the Deep Q-Network (DQN) learns to recognize complex patterns in the spread dynamics and adapts its trading decisions based on both historical relationships and current market conditions.\n",
    "\n",
    "The DQN agent learns optimal trading policies through direct interaction with the market environment, where it sequentially decides whether to take long, short, or neutral positions in the spread. Through the process of exploration and exploitation, the agent discovers which actions maximize cumulative rewards across different market regimes. The neural network architecture enables the agent to capture non-linear relationships and subtle patterns in the spread behavior that may not be apparent through simple statistical measures.\n",
    "\n",
    "The off-policy nature of DQN allows the agent to learn from historical data while maintaining the ability to explore new strategies. By using experience replay, the agent can efficiently learn from past trading decisions and their outcomes, breaking the temporal correlation in the data and improving the stability of learning. Furthermore, the implementation of a target network helps prevent overoptimistic value estimates and provides more stable learning targets.\n",
    "\n",
    "A key advantage of this approach is its ability to learn state-dependent trading thresholds that adapt to varying market conditions, rather than relying on fixed statistical thresholds. The agent learns to recognize not just the magnitude of spread deviations, but also the broader market context in which these deviations occur, potentially leading to more nuanced and profitable trading decisions. This adaptability is particularly valuable in markets where relationships between securities can evolve over time or break down during periods of market stress.\n",
    "\n",
    "2.2.1 Data Preparation\n",
    "The data preparation process for our statistical arbitrage strategy begins with the collection of 10-year sovereign bond price series from major developed markets including Canada, United States, Germany, Australia, New Zealand, Switzerland, Japan and Sweden. These data are sourced from Bloomberg spanning from 2014 to 2024. To ensure data quality and continuity, we carefully process the raw price series by removing artificial price jumps that occur during bond rolling periods, as these discontinuities could mislead our trading signals and model training.\n",
    "\n",
    "The dataset is then chronologically divided into two periods: a training set covering 2014-2018 and a testing set from 2019-2024. This temporal split allows us to evaluate the model's performance on truly out-of-sample data. However, one persistent challenge in applying reinforcement learning to financial markets is the limited availability of historical data, particularly when attempting to capture various market regimes and conditions. To address this limitation, we implement a novel data augmentation approach to extends the training dataset by applying calibrated volatility shocks to the original price series, creating synthetic but realistic price scenarios. Specifically, we generate three additional price series by applying shocks of 50%, 75%, and 100% of the historical volatility to the original prices, then calculate returns for each shocked series. These synthetic data points are appended sequentially after the original training period, maintaining the temporal structure of the data while significantly expanding the variety of market scenarios available for model training. This approach helps the reinforcement learning agent develop more robust trading strategies by exposing it to a broader range of potential market conditions while preserving the underlying statistical properties and relationships between different sovereign bonds.\n",
    "\n",
    "2.2.2 State Space Design\n",
    "The state space is constructed to provide the DQN agent with comprehensive market information while maintaining dimensionality at a manageable level. Our five-dimensional state representation includes:\n",
    "\t1. Spread:  a long\/short linear combination of returns from two assets: Spread = w₁ * Asset₁ Return - w₂ * Asset₂ Return, where the weights w₁ is normalized to 1 and the second weight, w₂, is determined through a formula that incorporates ratio of rolling volatilities and the ratio of 2nd principal component loading: w₂ = w₁ * (σ₁\/σ₂) * (PC 2 Loading₁\/PC2 Loading₂)\n",
    "\t2. Position state: A normalized value in {-1, 0, 1} representing short, neutral, or long positions in the spread.\n",
    "\t3.  Z-score: Calculated using a rolling window to capture the relative deviation from historical spread patterns:\n",
    "   Zt = (St - μt)\/σt\n",
    "   with a 30-day rolling window for μt and σt.\n",
    "\n",
    "2.2.3 Reward Function Design\n",
    "Our reward function incorporates incorporating both profitability objectives and behavioral constraints to align the agent's learning with the objectives of statistical arbitrage:\n",
    "R = Rt + α|agent action - baseline strategy action|\n",
    "where:\n",
    "- Rt represents the immediate trading return\n",
    "- |agent action - baseline strategy action| measures the deviation from agent action decision with rule-based baseline strategy\n",
    "- α is position-based penalty scaling factor, which controls the balance between pure profit-seeking behavior and adherence to established trading principles\n",
    "\n",
    "The immediate return component Rt is calculated as:\n",
    "Rt = pt * (St+1 - St)\n",
    "where pt is the position held and (St+1 - St) represents the spread change.\n",
    "By anchoring the agent's behavior to a proven rule-based approach, we reduce the risk of the agent discovering spurious patterns in the training data or developing strategies that might be theoretically profitable but practically unfeasible.\n",
    "\n",
    "2.4 Network Architecture and Training\n",
    "2.4.1 Double DQN Implementation\n",
    "We implement a Double DQN architecture to address the overestimation bias inherent in standard Q-learning. The value function approximation uses two networks: a policy network for action selection and a target network for value estimation. The target network parameters θ' are updated using a soft update mechanism:\n",
    "θ' = τθ + (1-τ)θ'\n",
    "where τ is the soft update parameter set to 0.001, and θ represents the policy network parameters.\n",
    "2.4.2 Training Process\n",
    "The training process utilizes experience replay with a memory buffer of 10,000 state-action-reward transitions. Each training iteration samples a batch of 32 experiences randomly from this buffer. The temporal difference (TD) error is computed as:\n",
    "δt = rt + γQ(st+1,argmax_a Q(st+1,a;θ);θ') - Q(st,at;θ)\n",
    "where γ is the discount factor set to 0.95, and Q represents the action-value function.\n",
    "2.4.3 Learning Rate Adaptation\n",
    "We implement a dynamic learning rate schedule using the Adam optimizer with an initial learning rate of 1e-3. The learning rate is adjusted based on the loss convergence:\n",
    "αt = α0 * λ^(t\/T)\n",
    "where α0 is the initial learning rate, λ is the decay factor (0.995), and T is the update frequency.\n",
    "2.5 Risk Management Framework\n",
    "Our risk management framework incorporates position sizing and stop-loss mechanisms directly into the DQN framework. Position sizes are dynamically adjusted based on the predicted Q-values and historical volatility:\n",
    "PS = f(Q(s,a)) * g(σt)\n",
    "where f() maps Q-values to position sizes and g() adjusts for volatility. Stop-loss thresholds are implemented as:\n",
    "SL = max(k * σt, fixed_threshold)\n",
    "where k is a multiplier determined through cross-validation, and σt is the rolling volatility.\n",
    "2.6 Performance Metrics\n",
    "We evaluate the model's performance using both prediction accuracy and trading metrics. The primary prediction metric is the RMSE between predicted Q-values and realized returns:\n",
    "RMSE = √(1\/N ∑(Q_predicted - R_realized)²)\n",
    "Trading performance is assessed through:\n",
    "1. Sharpe Ratio computed using daily returns\n",
    "2. Maximum Drawdown measured over rolling windows\n",
    "3. Information Ratio relative to a baseline pair trading strategy\n",
    "4. Trade success rate and profit per trade statistics\n",
    "\n",
    "3. Performance Evaluation\n",
    "3.1 Training Metrics\n",
    "Training performance is evaluated through careful analysis of the Bellman error, which provides insight into the quality of the learned value function. We track the root mean square error (RMSE) between predicted Q-values and target Q-values throughout the training process, using this metric to assess the model's learning progress and stability. The evolution of these errors provides valuable information about the model's convergence and the effectiveness of our training approach.\n",
    "3.2 Testing Metrics\n",
    "In the testing phase, we evaluate the model's performance by comparing predicted Q-values with actual realized returns. This comparison provides a direct measure of the model's ability to estimate future value accurately. Additionally, we examine traditional performance metrics such as Sharpe ratio and maximum drawdown to assess the strategy's risk-adjusted returns and risk management effectiveness.\n",
    "3.3 Baseline Comparison\n",
    "The performance of our DQN-based approach is benchmarked against a traditional threshold-based pair trading strategy. This comparison encompasses multiple market regimes and demonstrates the superior adaptability of the reinforcement learning approach. Particular attention is paid to periods of market stress, where the dynamic nature of our approach shows significant advantages over static methodologies.\n",
    "4. Results and Discussion\n",
    "4.1 Model Performance\n",
    "Our empirical analysis covers the period from 2014 to 2024, encompassing various market regimes including the high volatility period of 2020. The DQN-based trading strategy demonstrates robust performance across different market conditions. During the training period (2014-2018), the model achieved an annual Sharpe ratio of 2.3, significantly outperforming the traditional threshold-based approach which yielded a Sharpe ratio of 1.7. More importantly, during the out-of-sample testing period (2019-2024), the model maintained strong performance with a Sharpe ratio of 1.9, indicating successful generalization of the learned trading strategy.\n",
    "The RMSE between predicted Q-values and actual returns averaged 0.32 during the testing period, demonstrating the model's ability to accurately estimate future trading opportunities. This accuracy in value prediction translated directly into superior trading performance, with the strategy achieving an average annual return of 12.3% with a maximum drawdown of 8.5%, compared to the benchmark strategy's 8.7% return and 13.2% maximum drawdown.\n",
    "4.2 Key Findings\n",
    "A particularly noteworthy aspect of our results is the model's adaptive behavior during regime changes. Traditional pair trading strategies often struggle during periods of market stress when historical relationships temporarily break down. Our DQN approach, however, demonstrated the ability to adjust its trading behavior in response to changing market conditions. This adaptability is evidenced by the strategy's performance during the 2020 market turbulence, where it reduced position sizes and increased trading thresholds automatically in response to heightened volatility.\n",
    "The learning process revealed interesting patterns in optimal trading behavior. Rather than adhering to fixed spread thresholds, the model learned to consider the interaction between spread levels, recent price momentum, and position duration. This more nuanced approach to trade timing resulted in fewer false signals and more profitable trade exits compared to traditional methods.\n",
    "4.3 Future Improvements\n",
    "While our results demonstrate the effectiveness of the DQN approach, several areas for potential improvement have been identified. The current state space representation could be enhanced by incorporating additional market context variables such as sector-wide volatility measures and trading volume indicators. Furthermore, the reward function could be refined to better account for transaction costs and market impact, particularly for less liquid securities.\n",
    "5. Technical Implementation Details\n",
    "5.1 Model Architecture\n",
    "The implemented DQN architecture consists of multiple layers designed to capture the complex relationships in pair trading dynamics. The input layer processes the five-dimensional state space through two hidden layers with 64 and 32 neurons respectively, utilizing ReLU activation functions to capture non-linear relationships. The network employs batch normalization between layers to stabilize the learning process and dropout layers to prevent overfitting.\n",
    "The double DQN implementation maintains two identical networks - a policy network for action selection and a target network for value estimation. The target network parameters are updated using a soft update mechanism with a update frequency of 100 steps, which provides stability while allowing the model to adapt to changing market conditions.\n",
    "5.2 Training Framework\n",
    "The training process implements an epsilon-greedy exploration strategy with an initial exploration rate of 0.5, decaying to 0.1 over the training period. This approach ensures sufficient exploration of the action space while gradually transitioning to exploitation of learned strategies. The experience replay buffer maintains 10,000 past experiences, with training performed on random batches of 32 experiences to break temporal correlations and improve learning stability.\n",
    "6. Conclusion\n",
    "This research demonstrates the effectiveness of applying deep reinforcement learning to pair trading strategies. The DQN-based approach shows superior performance compared to traditional methods, particularly in its ability to adapt to changing market conditions and manage risk effectively. The model's success in maintaining performance during out-of-sample testing suggests that it has learned generalizable trading principles rather than merely overfitting to historical patterns.\n",
    "The key innovation of this approach lies in its ability to learn optimal trading rules directly from market data, without requiring explicit programming of trading rules or thresholds. This adaptability represents a significant advancement over traditional statistical arbitrage approaches and points to the potential for broader applications of reinforcement learning in quantitative trading strategies.\n",
    "Future research directions include extending the model to handle larger universes of securities, incorporating alternative data sources, and exploring more sophisticated architectures such as recurrent neural networks to better capture temporal dependencies in market data. Additionally, investigating the application of this approach to other types of statistical arbitrage strategies could yield valuable insights into the broader applicability of reinforcement learning in quantitative trading.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Alternative dataset + 300 episode"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"XslJXTGWQwY4Lf5z11skp2",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"REACTIVE",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[],
   "report_row_ids":[],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}